{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "building-KantaiBERT-from-scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/transformers-for-natural-language-processing/blob/main/3-pretraining-RoBERTa-model-from-scratch/building_KantaiBERT_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1oqh0F6W3ad"
      },
      "source": [
        "## Building KantaiBERT from scratch using Transformers and Tokenizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y-FvagMdFtL"
      },
      "source": [
        "The Transformer model of this Notebook is a Transformer model named ***KantaiBERT***. ***KantaiBERT*** is trained as a RoBERTa Transformer with DistilBERT architecture. The dataset was compiled with three books by Immanuel Kant downloaded from the [Gutenberg Project](https://www.gutenberg.org/). \n",
        "\n",
        "<center><img src=\"https://eco-ai-horizons.com/data/Kant.jpg\" style=\"margin: auto; display: block; width: 260px;\"></center>\n",
        "\n",
        "![](https://commons.wikimedia.org/wiki/Kant_gemaelde_1.jpg)\n",
        "\n",
        "***KantaiBERT*** was pretrained with a small model of 84 million parameters using the same number of layers and heads as DistilBert, i.e., 6 layers, 768 hidden size,and 12 attention heads. ***KantaiBERT*** is then fine-tuned for a downstream masked Language Modeling task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFILYEDbdcFG"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUD724uhdg8_"
      },
      "source": [
        "Notebook edition ([link to original of the reference blogpost](https://huggingface.co/blog/how-to-train)).\n",
        "\n",
        "We will need to install Hugging Face transformers and tokenizers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5duRggBRZKvP"
      },
      "source": [
        "# We won't need TensorFlow here\n",
        "!pip uninstall -y tensorflow\n",
        "# Install `transformers` from master\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip list | grep -E 'transformers|tokenizers'\n",
        "# transformers version at notebook update --- 2.9.1\n",
        "# tokenizers version at notebook update --- 0.7.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXYVqKV-FWWU"
      },
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "\n",
        "from transformers import RobertaConfig, RobertaTokenizer, RobertaForMaskedLM\n",
        "from transformers import LineByLineTextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments, pipeline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-di6zgTfEmD"
      },
      "source": [
        "## Step 1: Loading the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBYwm-hJf36M"
      },
      "source": [
        "I chose to use the works of Immanuel Kant (1724-1804), the German philosopher, who was the epitome of the Age of Enlightenment. The idea is to introduce human-like logic and pretrained reasoning for downstream reasoning tasks.\n",
        "\n",
        "I compiled the following three books by Immanuel Kant into a text file named `kant.txt`:\n",
        "\n",
        "- The Critique of Pure Reason\n",
        "- The Critique of Practical Reason\n",
        "- Fundamental Principles of the Metaphysic of Morals\n",
        "\n",
        "kant.txt provides a small training dataset to train the transformer model. The result obtained remains experimental. For a real-life project, I would\n",
        "add the complete works of Immanuel Kant, Rene Descartes, Pascal, and Leibnitz, for example.\n",
        "\n",
        "The text file contains the raw text of the books:\n",
        "\n",
        "```\n",
        "â€¦For it is in reality vain to profess _indifference_ in regard to such\n",
        "inquiries, the object of which cannot be indifferent to humanity.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1pUFHnefF1u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "210762e7-42a4-4878-89f2-a43031604ea4"
      },
      "source": [
        "#1.Load kant.txt using the Colab file manager\n",
        "#2.Downloading the file from GitHub\n",
        "!curl -L https://github.com/rahiakela/transformers-for-natural-language-processing/raw/main/3-pretraining-RoBERTa-model-from-scratch/kant.txt --output \"kant.txt\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   209  100   209    0     0    487      0 --:--:-- --:--:-- --:--:--   487\n",
            "100 10.7M  100 10.7M    0     0  6832k      0  0:00:01  0:00:01 --:--:-- 32.3M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XE-zq89fjIE"
      },
      "source": [
        "## Step 3: Training a Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdAbCELfit3y"
      },
      "source": [
        "Since we does not use a pretrained tokenizer. For example, a pretrained GPT-2 tokenizer could be used. However, the training process includes training a tokenizer from scratch.\n",
        "\n",
        "Hugging Face's `ByteLevelBPETokenizer()` will be trained using `kant.txt`. A bytelevel tokenizer will break a string or word down into a sub-string or sub-word.\n",
        "\n",
        "There are two main advantages among many others:\n",
        "\n",
        "- The tokenizer can break words into minimal components. Then it will merge\n",
        "these small components into statistically interesting ones. For example,\n",
        "\"smaller\" and smallest\" can become \"small,\" \"er,\" and \"est.\" The tokenizer\n",
        "can go further, and we could obtain \"sm\" and \"all,\" for example. In any case,\n",
        "the words are broken down into sub-word tokens and smaller units of subword\n",
        "parts such as \"sm\" and \"all\" instead of simply \"small.\"\n",
        "\n",
        "- The chunks of strings classified as an unknown `unk_token`, using WorkPiece\n",
        "level encoding, will practically disappear."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMnymRDLe0hi",
        "outputId": "41eb435f-fcda-4aa0-90d6-8870b923a3b6"
      },
      "source": [
        "%%time \n",
        "\n",
        "paths = [str(x) for x in Path(\".\").glob(\"**/*.txt\")]\n",
        "# Initialize a tokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "# Customize training\n",
        "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
        "    \"<s>\",\n",
        "    \"<pad>\",\n",
        "    \"</s>\",\n",
        "    \"<unk>\",\n",
        "    \"<mask>\",\n",
        "])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 5.7 s, sys: 401 ms, total: 6.1 s\n",
            "Wall time: 1.64 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuDM7uInlBMG"
      },
      "source": [
        "The tokenizer will be trained to generate merged sub-string tokens and analyze their frequency.\n",
        "\n",
        "Let's take these two words in the middle of a sentence:\n",
        "\n",
        "```\n",
        "â€¦the tokenizerâ€¦\n",
        "```\n",
        "\n",
        "The first step will be to tokenize the string:\n",
        "\n",
        "```\n",
        "'Ä the', 'Ä token', 'izer',\n",
        "```\n",
        "\n",
        "The string is now tokenized into tokens with Ä  (whitespace) information.\n",
        "\n",
        "The next step is to replace them with their indices:\n",
        "\n",
        "| 'Ä the' | 'Ä token' | 'izer' |\n",
        "| ---    | ------   | ----   |\n",
        "| 150    | 5430     | 4712   |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcNN98lil715"
      },
      "source": [
        "## Step 4: Saving the files to disk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KAfCLLHnln_"
      },
      "source": [
        "The tokenizer will generate two files when trained:\n",
        "\n",
        "- `merges.txt`, which contains the merged tokenized sub-strings\n",
        "- `vocab.json`, which contains the indices of the tokenized sub-strings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqYKX1XYyRI-",
        "outputId": "f7445644-b4d0-45c0-afa6-752e5177e166"
      },
      "source": [
        "token_dir = '/content/KantaiBERT'\n",
        "if not os.path.exists(token_dir):\n",
        "  os.makedirs(token_dir)\n",
        "tokenizer.save_model('KantaiBERT')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['KantaiBERT/vocab.json', 'KantaiBERT/merges.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iI9i_fqtofPM"
      },
      "source": [
        "##  Step 5 Loading the Trained Tokenizer Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7pMWyFtogOt"
      },
      "source": [
        "We could have loaded pretrained tokenizer files. However, we trained our own\n",
        "tokenizer and now are ready to load the files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKVWB8WShT-z"
      },
      "source": [
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    \"./KantaiBERT/vocab.json\",\n",
        "    \"./KantaiBERT/merges.txt\"\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ii-sm3BJGhZN"
      },
      "source": [
        "The tokenizer can encode a sequence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9hQqVS_qZWg",
        "outputId": "d2acac33-f5db-402d-de97-fe6a39c46361"
      },
      "source": [
        "tokenizer.encode(\"The Critique of Pure Reason.\").tokens"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'Ä Critique', 'Ä of', 'Ä Pure', 'Ä Reason', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G43rdmMbGwTc"
      },
      "source": [
        "We can also ask to see the number of tokens in this sequence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGjAwZVGrfyS",
        "outputId": "be2bacfb-3888-4f93-ad02-d54d80062b20"
      },
      "source": [
        "tokenizer.encode(\"The Critique of Pure Reason.\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoding(num_tokens=6, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2NnA_gOG4G3"
      },
      "source": [
        "The tokenizer now processes the tokens to fit the BERT model variant used. The post processor will add a start and end token, for example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO5M3vrAhcuj"
      },
      "source": [
        "tokenizer._tokenizer.post_processor = BertProcessing(\n",
        "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
        "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
        ")\n",
        "\n",
        "tokenizer.enable_truncation(max_length=512)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWa8-isaHvwQ"
      },
      "source": [
        "Let's encode a post-processed sequence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcRkIdnJHwOk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "321504e6-c8b8-4958-85bb-b49b996b58e4"
      },
      "source": [
        "tokenizer.encode(\"The Critique of Pure Reason.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoding(num_tokens=8, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZCSz_GEH6vC"
      },
      "source": [
        "If we want to see what was added, we can ask the tokenizer to encode the postprocessed sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxokZoF_H8x3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1423ce33-012a-44ef-89a0-ba7e2cc9da12"
      },
      "source": [
        "tokenizer.encode(\"The Critique of Pure Reason.\").tokens"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>', 'The', 'Ä Critique', 'Ä of', 'Ä Pure', 'Ä Reason', '.', '</s>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3Qii9hCIEO9"
      },
      "source": [
        "The output shows that the start and end tokens have been added, which brings the number of tokens to 8 including start and end tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OPFYGG1ILJc"
      },
      "source": [
        "## Step 6: Checking Resource Constraints: GPU and NVIDIA "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0SznLqOIML_"
      },
      "source": [
        "KantaiBERT runs at optimal speed with a Graphics Processing Unit (GPU).\n",
        "\n",
        "We will first run a command to see if an NVIDIA GPU card is present:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kD140sFjh0LQ",
        "outputId": "fe9edbf4-583e-433d-a3f4-514eb5dd2873"
      },
      "source": [
        "# Checking Resource Constraints: GPU and NVIDIA \n",
        "!nvidia-smi"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Apr  2 06:41:28 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nrL_ACjIgU7"
      },
      "source": [
        "We will now check to make sure PyTorch sees CUDA:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNZZs-r6iKAV",
        "outputId": "2ecaa145-9543-4ac5-c695-d56191572104"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ibcXdlYInxX"
      },
      "source": [
        "Compute Unified Device Architecture (CUDA) was developed by NVIDIA to use\n",
        "the parallel computing power of its NVIDIA card."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVOWNZCIIob-"
      },
      "source": [
        "## Step 7: Defining the configuration of the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pYVGTMII0n7"
      },
      "source": [
        "We will be pretraining a RoBERTa-type transformer model using the same number\n",
        "of layers and heads as a DistilBERT transformer. The model will have a vocabulary size set to 52,000, 12 attention heads, and 6 layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTXXutqeDzPi"
      },
      "source": [
        "# Defining the configuration of the Model\n",
        "config = RobertaConfig(\n",
        "    vocab_size=52_000,\n",
        "    max_position_embeddings=514,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=6,\n",
        "    type_vocab_size=1,\n",
        ")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-UsuK9Ps0H7",
        "outputId": "6d0ae2f1-5021-4188-9592-75ceb5c6f686"
      },
      "source": [
        "print(config)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RobertaConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVyI3gPuI-9X"
      },
      "source": [
        "## Step 8: Re-creating the Tokenizer in Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idze-C0PKMD0"
      },
      "source": [
        "We are now ready to load our trained tokenizer, which is our pretrained tokenizer in `RobertaTokenizer.from_pretained()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4keFBUjQFOD1"
      },
      "source": [
        "# Re-creating the Tokenizer in Transformers\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"./KantaiBERT\", max_length=512)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ildt3MdoKT64"
      },
      "source": [
        "## Step 9: Initializing a Model From Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HFU-TA0Khgf"
      },
      "source": [
        "we will initialize a model from scratch and examine the size of the\n",
        "model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzMqR-dzF4Ro",
        "outputId": "5c2642b6-510c-4955-8e66-ccc371ffc882"
      },
      "source": [
        "# Initializing a Model From Scratch\n",
        "model = RobertaForMaskedLM(config=config)\n",
        "# If we print the model, we can see that it is a BERT model with 6 layers and 12 heads\n",
        "print(model)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RobertaForMaskedLM(\n",
            "  (roberta): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(52000, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): RobertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (lm_head): RobertaLMHead(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "    (decoder): Linear(in_features=768, out_features=52000, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwtvsuEaLPbJ"
      },
      "source": [
        "The model is small and contains 83,504,416 parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jU6JhBSTKiaM",
        "outputId": "99ce79b0-bd1a-4015-b797-b7424505be54"
      },
      "source": [
        "print(model.num_parameters())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "83504416\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkBGRX3NLfWH"
      },
      "source": [
        "**Exploring the Parameters**\n",
        "\n",
        "Let's now look into the parameters. We first store the parameters in LP and calculate the length of the list of parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BXhhe7twTxb"
      },
      "source": [
        "# Exploring the Parameters\n",
        "LP=list(model.parameters())\n",
        "lp=len(LP)\n",
        "print(lp)\n",
        "for p in range(0,lp):\n",
        "  print(LP[p])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjiUxjDoLyMm"
      },
      "source": [
        "The output shows that there are approximately 106 matrices and vectors, which\n",
        "might vary from one transformer model to another.\n",
        "\n",
        "**Counting the parameters**\n",
        "\n",
        "The number of parameters is calculated by taking all parameters in the model and adding them up; for example:\n",
        "\n",
        "- The vocabulary (52,000) x dimensions (768)\n",
        "- The size of many vectors is 1 x 768\n",
        "- The many other dimensions found\n",
        "\n",
        "You will note that $d_{model} = 768$. There are 12 heads in the model. The dimension of $d_k$ for each head will thus be $d_k = \\frac{d_{model}}{12} = 64$. This shows, once again, the optimized\n",
        "Lego concept of the building blocks of a transformer.\n",
        "\n",
        "We will take this further and count the number of parameters of each tensor.\n",
        "\n",
        "First, the program initializes a parameter counter named np (number of parameters) and goes through the lp (108) number of elements in the list of parameters.\n",
        "\n",
        "The parameters are matrices and vectors of different sizes; for example.\n",
        "- 768 x 768\n",
        "- 768 x 1\n",
        "- 768\n",
        "\n",
        "We can see that some parameters are two-dimensional, and some are onedimensional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ej82kG6K3akQ",
        "outputId": "7c7d4d3f-683f-4286-e36b-71e1bd7dced4"
      },
      "source": [
        "# Counting the parameters\n",
        "np=0\n",
        "for p in range(0, lp):#number of tensors\n",
        "  # An easy way to find out is to try and see if a parameter p in the list LP[p] has two dimensions or not\n",
        "  PL2=True\n",
        "  try:\n",
        "    L2=len(LP[p][0]) #check if 2D\n",
        "  except:\n",
        "    L2=1             #not 2D but 1D\n",
        "    PL2=False\n",
        "  # L1 is the size of the first dimension of the parameter. L3 is the size of the parameters defined by  \n",
        "  L1=len(LP[p])      \n",
        "  L3=L1*L2\n",
        "  # We can now add the parameters up at each step of the loop\n",
        "  np+=L3             # number of parameters per tensor\n",
        "\n",
        "  \"\"\"\n",
        "  We will obtain the sum of the parameters, but we also want to see exactly how the\n",
        "  number of parameters of a transformer model is calculated\n",
        "  \"\"\"\n",
        "  if PL2==True:\n",
        "    print(p,L1,L2,L3)  # displaying the sizes of the parameters\n",
        "  if PL2==False:\n",
        "    print(p,L1,L3)  # displaying the sizes of the parameters\n",
        "\n",
        "print(np)              # total number of parameters"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 52000 768 39936000\n",
            "1 514 768 394752\n",
            "2 1 768 768\n",
            "3 768 768\n",
            "4 768 768\n",
            "5 768 768 589824\n",
            "6 768 768\n",
            "7 768 768 589824\n",
            "8 768 768\n",
            "9 768 768 589824\n",
            "10 768 768\n",
            "11 768 768 589824\n",
            "12 768 768\n",
            "13 768 768\n",
            "14 768 768\n",
            "15 3072 768 2359296\n",
            "16 3072 3072\n",
            "17 768 3072 2359296\n",
            "18 768 768\n",
            "19 768 768\n",
            "20 768 768\n",
            "21 768 768 589824\n",
            "22 768 768\n",
            "23 768 768 589824\n",
            "24 768 768\n",
            "25 768 768 589824\n",
            "26 768 768\n",
            "27 768 768 589824\n",
            "28 768 768\n",
            "29 768 768\n",
            "30 768 768\n",
            "31 3072 768 2359296\n",
            "32 3072 3072\n",
            "33 768 3072 2359296\n",
            "34 768 768\n",
            "35 768 768\n",
            "36 768 768\n",
            "37 768 768 589824\n",
            "38 768 768\n",
            "39 768 768 589824\n",
            "40 768 768\n",
            "41 768 768 589824\n",
            "42 768 768\n",
            "43 768 768 589824\n",
            "44 768 768\n",
            "45 768 768\n",
            "46 768 768\n",
            "47 3072 768 2359296\n",
            "48 3072 3072\n",
            "49 768 3072 2359296\n",
            "50 768 768\n",
            "51 768 768\n",
            "52 768 768\n",
            "53 768 768 589824\n",
            "54 768 768\n",
            "55 768 768 589824\n",
            "56 768 768\n",
            "57 768 768 589824\n",
            "58 768 768\n",
            "59 768 768 589824\n",
            "60 768 768\n",
            "61 768 768\n",
            "62 768 768\n",
            "63 3072 768 2359296\n",
            "64 3072 3072\n",
            "65 768 3072 2359296\n",
            "66 768 768\n",
            "67 768 768\n",
            "68 768 768\n",
            "69 768 768 589824\n",
            "70 768 768\n",
            "71 768 768 589824\n",
            "72 768 768\n",
            "73 768 768 589824\n",
            "74 768 768\n",
            "75 768 768 589824\n",
            "76 768 768\n",
            "77 768 768\n",
            "78 768 768\n",
            "79 3072 768 2359296\n",
            "80 3072 3072\n",
            "81 768 3072 2359296\n",
            "82 768 768\n",
            "83 768 768\n",
            "84 768 768\n",
            "85 768 768 589824\n",
            "86 768 768\n",
            "87 768 768 589824\n",
            "88 768 768\n",
            "89 768 768 589824\n",
            "90 768 768\n",
            "91 768 768 589824\n",
            "92 768 768\n",
            "93 768 768\n",
            "94 768 768\n",
            "95 3072 768 2359296\n",
            "96 3072 3072\n",
            "97 768 3072 2359296\n",
            "98 768 768\n",
            "99 768 768\n",
            "100 768 768\n",
            "101 52000 52000\n",
            "102 768 768 589824\n",
            "103 768 768\n",
            "104 768 768\n",
            "105 768 768\n",
            "83504416\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgJZ3d5BRAMX"
      },
      "source": [
        "Note that if a parameter only has one dimension, `PL2=False`, then we only display the first dimension.\n",
        "\n",
        "The total number of parameters of the RoBERTa model is displayed at the end of\n",
        "the list: `83,504,416`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYTpHscBRWhw"
      },
      "source": [
        "## Step 10: Building the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS0tFQnFRXh5"
      },
      "source": [
        "We will now load the dataset line by line for batch training with block_\n",
        "size=128 limiting the length of an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlvP_A-THEEl",
        "outputId": "a28470ea-ddb9-4639-e039-99cc777e7848"
      },
      "source": [
        "# Step 10: Building the Dataset\n",
        "%%time\n",
        "\n",
        "dataset = LineByLineTextDataset(tokenizer=tokenizer, file_path=\"./kant.txt\", block_size=128)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:124: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 22 s, sys: 369 ms, total: 22.4 s\n",
            "Wall time: 22.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUbSbCTVU3iO"
      },
      "source": [
        "The output shows that Hugging Face has invested a considerable amount of\n",
        "resources into optimizing the time it takes to process data.\n",
        "\n",
        "We will now define a data collator to create an object for backpropagation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h1mlm2MVBVA"
      },
      "source": [
        "## Step 11: Defining a Data Collator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3yyHZNZVCNT"
      },
      "source": [
        "We need to run a data collator before initializing the trainer. A data collator will take samples from the dataset and collate them into batches. The results are dictionarylike objects.\n",
        "\n",
        "We are preparing a batched sample process for **Masked Language Modeling (MLM)** by setting `mlm=True`.\n",
        "\n",
        "We also set the number of masked tokens to train `mlm_probability=0.15`. This will determine the percentage of tokens masked during the pretraining process.\n",
        "\n",
        "\n",
        "We now initialize data_collator with our tokenizer, MLM activated, and the\n",
        "proportion of masked tokens set to 0.15:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTgWPa9Dipk2"
      },
      "source": [
        "# Defining a Data Collator\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDOmxmB5Vu9G"
      },
      "source": [
        "## Step 12: Initializing the Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK6R5sxoVv1A"
      },
      "source": [
        "We can now initialize the trainer. For educational purposes, the program\n",
        "trains the model quickly. The number of epochs is limited to one. The GPU comes in handy since we can share the batches and multi-process the training tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpvnFFmZJD-N"
      },
      "source": [
        "# Initializing the Trainer\n",
        "training_args = TrainingArguments(output_dir=\"./KantaiBERT\", \n",
        "                                  overwrite_output_dir=True, \n",
        "                                  num_train_epochs=1, \n",
        "                                  per_device_train_batch_size=64,\n",
        "                                  save_steps=10_000, \n",
        "                                  save_total_limit=2)\n",
        "\n",
        "trainer = Trainer(model=model, args=training_args, data_collator=data_collator, train_dataset=dataset)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2CXzpb3XHRy"
      },
      "source": [
        "The model is now ready for training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfnZ5T-IXLEZ"
      },
      "source": [
        "## Step 13: Pre-training the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WFBuOLUXL3Y"
      },
      "source": [
        "Everything is ready. The trainer is launched with one line of code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "VmaHZXzmkNtJ",
        "outputId": "9d7848b3-8fa2-4b92-f285-08fcff708b21"
      },
      "source": [
        "# Pre-training the Model\n",
        "%%time\n",
        "\n",
        "trainer.train()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='2672' max='2672' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2672/2672 07:06, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>5.474200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>4.025700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>3.732400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>3.514800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>3.402000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 8min 35s, sys: 4min 27s, total: 13min 2s\n",
            "Wall time: 7min 7s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2672, training_loss=3.9887362268870463, metrics={'train_runtime': 426.5652, 'train_samples_per_second': 6.264, 'total_flos': 1689347110470912.0, 'epoch': 1.0, 'init_mem_cpu_alloc_delta': 2077216768, 'init_mem_gpu_alloc_delta': 334180352, 'init_mem_cpu_peaked_delta': 0, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 15331328, 'train_mem_gpu_alloc_delta': 1010226176, 'train_mem_cpu_peaked_delta': 0, 'train_mem_gpu_peaked_delta': 2583970816})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-doUtv1XWRu"
      },
      "source": [
        "The output displays the training process in real time showing the loss, learning rate, epoch, and steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Njuyd6m-XkMn"
      },
      "source": [
        "## Step 14: Saving the Final Model(+tokenizer + config) to disk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAb1GGKXXk5q"
      },
      "source": [
        "The model has been trained. It's time to save our work.\n",
        "\n",
        "We will now save the model and configuration:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDNgPls7_l13"
      },
      "source": [
        "# Saving the Final Model(+tokenizer + config) to disk\n",
        "trainer.save_model(\"./KantaiBERT\")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqbX5douX1e2"
      },
      "source": [
        "`config.json, pytorh_model.bin`, and `training_args.bin` should now appear in the file manager.\n",
        "\n",
        "`merges.txt` and `vocab.json` contain the pretrained tokenization of the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1vfLECdZDj_"
      },
      "source": [
        "## Step 15: Language Modeling with the FillMaskPipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVO1RffUZEXN"
      },
      "source": [
        "We have built a model from scratch.\n",
        "\n",
        "Let's import the pipeline to perform a language modeling task with our pretrained model and tokenizer.\n",
        "\n",
        "We will now import a language modeling fill-mask task. We will use our trained\n",
        "model and trained tokenizer to perform masked language modeling:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltXgXyCbAJLY"
      },
      "source": [
        "# Language Modeling with the FillMaskPipeline\n",
        "fill_mask = pipeline(\"fill-mask\", model=\"./KantaiBERT\", tokenizer=\"./KantaiBERT\")"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJzeKibyaJQD"
      },
      "source": [
        "We can now ask our model to think like Immanuel Kant:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIvgZ3S6AO0z",
        "outputId": "4b0ca9bd-df9c-43ec-e3d2-069843342a0a"
      },
      "source": [
        "fill_mask(\"Human thinking involves human <mask>.\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.040765952318906784,\n",
              "  'sequence': 'Human thinking involves human reason.',\n",
              "  'token': 393,\n",
              "  'token_str': ' reason'},\n",
              " {'score': 0.017752638086676598,\n",
              "  'sequence': 'Human thinking involves human conceptions.',\n",
              "  'token': 605,\n",
              "  'token_str': ' conceptions'},\n",
              " {'score': 0.01720724254846573,\n",
              "  'sequence': 'Human thinking involves human understanding.',\n",
              "  'token': 600,\n",
              "  'token_str': ' understanding'},\n",
              " {'score': 0.013491691090166569,\n",
              "  'sequence': 'Human thinking involves human experience.',\n",
              "  'token': 531,\n",
              "  'token_str': ' experience'},\n",
              " {'score': 0.012612631544470787,\n",
              "  'sequence': 'Human thinking involves human time.',\n",
              "  'token': 526,\n",
              "  'token_str': ' time'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bc2s76wFamY9"
      },
      "source": [
        "The output will likely change after each run because we are pretraining the model from scratch with a limited amount of data. However, the output obtained in this run is interesting because it introduces conceptional language modeling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPkfZxV_apDH",
        "outputId": "feca5b2c-ae0a-4bf0-b074-b206e7180543"
      },
      "source": [
        "fill_mask(\"How human show love <mask>.\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.013027912005782127,\n",
              "  'sequence': 'How human show love reason.',\n",
              "  'token': 393,\n",
              "  'token_str': ' reason'},\n",
              " {'score': 0.012774677015841007,\n",
              "  'sequence': 'How human show love is.',\n",
              "  'token': 300,\n",
              "  'token_str': ' is'},\n",
              " {'score': 0.012741762213408947,\n",
              "  'sequence': 'How human show love,.',\n",
              "  'token': 16,\n",
              "  'token_str': ','},\n",
              " {'score': 0.010261932387948036,\n",
              "  'sequence': 'How human show love conceptions.',\n",
              "  'token': 605,\n",
              "  'token_str': ' conceptions'},\n",
              " {'score': 0.008668892085552216,\n",
              "  'sequence': 'How human show love itself.',\n",
              "  'token': 500,\n",
              "  'token_str': ' itself'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_STgAMKaz1y",
        "outputId": "6ffd63b5-8f8a-4965-8fdd-3716a4c0d1ec"
      },
      "source": [
        "fill_mask(\"What is wealth of nations <mask>.\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.02068767324090004,\n",
              "  'sequence': 'What is wealth of nations reason.',\n",
              "  'token': 393,\n",
              "  'token_str': ' reason'},\n",
              " {'score': 0.018951253965497017,\n",
              "  'sequence': 'What is wealth of nations conceptions.',\n",
              "  'token': 605,\n",
              "  'token_str': ' conceptions'},\n",
              " {'score': 0.008094606921076775,\n",
              "  'sequence': 'What is wealth of nations conception.',\n",
              "  'token': 418,\n",
              "  'token_str': ' conception'},\n",
              " {'score': 0.007942058145999908,\n",
              "  'sequence': 'What is wealth of nations experience.',\n",
              "  'token': 531,\n",
              "  'token_str': ' experience'},\n",
              " {'score': 0.00781000591814518,\n",
              "  'sequence': 'What is wealth of nations unity.',\n",
              "  'token': 688,\n",
              "  'token_str': ' unity'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FonPcZEybTbX"
      },
      "source": [
        "The predictions might vary at each run and each time Hugging Face updates its\n",
        "models.\n",
        "\n",
        "However, the following output comes out often:\n",
        "\n",
        "```\n",
        "Human thinking involves human reason\n",
        "```\n",
        "\n",
        "**The goal here was to see how to train a transformer model. We can see that very interesting humanlike predictions can be made.**\n",
        "\n",
        "These results are experimental and subject to variations during the training process. They will change each time we train the model again.\n",
        "\n",
        "The model would require much more data from other `Age of Enlightenment thinkers`.\n",
        "\n",
        "**However, the goal of this model is to show that we can create datasets to train a transformer for a specific type of complex language modeling task.**\n",
        "\n",
        "Thanks to the Transformer, we are only at the beginning of a new era of AI!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDMlTBA2cs0D"
      },
      "source": [
        "## Next steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EZFFT-Ectre"
      },
      "source": [
        "You have trained a transformer from scratch. Take some time to imagine what you could do in your personal or corporate environment. You could create a dataset for a specific task and train it from scratch. Use your areas of interest or company projects to experiment with the fascinating world of transformer construction kits!\n",
        "\n",
        "Once you have made a model you like, you can share it with the Hugging Face\n",
        "community. Your model will appear on the Hugging Face models page: \n",
        "https://huggingface.co/models\n",
        "\n",
        "You can upload your model in a few steps using the instructions described on this page: https://huggingface.co/transformers/model_sharing.html\n",
        "\n",
        "You can also download models the Hugging Face community has shared to get\n",
        "new ideas for your personal and professional projects.\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}