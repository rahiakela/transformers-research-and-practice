{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3-architecture-of-transformer.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPqkfKasx6CtfMGjvTbIFY1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "50780f87f73a482884930e38f48a048b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a9c524dc924d4ceeb72cdc879f6a5e57",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_92e469deaaad453dab41aba22b7a29c4",
              "IPY_MODEL_61c87acb992a4d7087c7d84ca5dd280e"
            ]
          }
        },
        "a9c524dc924d4ceeb72cdc879f6a5e57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "92e469deaaad453dab41aba22b7a29c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3f1fbb70b63445828f4858ec8683ed26",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1199,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1199,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_516301f1d6084408b2dd492ba85eda4a"
          }
        },
        "61c87acb992a4d7087c7d84ca5dd280e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a805f3c5aaa549459dd33650f1c354d9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.20k/1.20k [00:15&lt;00:00, 76.7B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_09c4b3c66b8349bd88ab9c3018488191"
          }
        },
        "3f1fbb70b63445828f4858ec8683ed26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "516301f1d6084408b2dd492ba85eda4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a805f3c5aaa549459dd33650f1c354d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "09c4b3c66b8349bd88ab9c3018488191": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "485caf29bba544c7b40a8a2cc67b63fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e357e98bb02d45f284b6b4715557c3f4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e2b249311835466c940ed323960cdf50",
              "IPY_MODEL_9397616eda7a4b4390c1e381fe4149cb"
            ]
          }
        },
        "e357e98bb02d45f284b6b4715557c3f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e2b249311835466c940ed323960cdf50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f7a3169e2ac24c578bb04bd4d4384a48",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 891691430,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 891691430,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b1a007348cfe4c6fbbf3c2bbd7a6cea2"
          }
        },
        "9397616eda7a4b4390c1e381fe4149cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bdda70c36128462286650bad783efe5d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 892M/892M [00:14&lt;00:00, 59.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5571d04e945d4b35a20b2af24a5368de"
          }
        },
        "f7a3169e2ac24c578bb04bd4d4384a48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b1a007348cfe4c6fbbf3c2bbd7a6cea2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bdda70c36128462286650bad783efe5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5571d04e945d4b35a20b2af24a5368de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "77bbfb1084ef42789cc12744d7ad568e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_036450c1dde64ac5927293712c8c5194",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3bbe1781263548f0b3c2538d39280434",
              "IPY_MODEL_0b4e804209744998802e182f93315d58"
            ]
          }
        },
        "036450c1dde64ac5927293712c8c5194": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3bbe1781263548f0b3c2538d39280434": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_47cb867dbbf04b1a94cfdedeacc1a6af",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 791656,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 791656,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5fe8b25b627948fd8a3e67ef8a9a5af3"
          }
        },
        "0b4e804209744998802e182f93315d58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4d7e2853f08b442ca76a820d28f23080",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 792k/792k [00:02&lt;00:00, 378kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0437c9aa007c4ba4a72b9479df8f1ca7"
          }
        },
        "47cb867dbbf04b1a94cfdedeacc1a6af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5fe8b25b627948fd8a3e67ef8a9a5af3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4d7e2853f08b442ca76a820d28f23080": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0437c9aa007c4ba4a72b9479df8f1ca7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "33603decb5a949b0ba6561ed085c9517": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_58212e94fa634e7db3f8fa0dfa16ac45",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9d78bd32702c4b678d20827692bf42cb",
              "IPY_MODEL_262fb513ad47429581d80f2a47a99424"
            ]
          }
        },
        "58212e94fa634e7db3f8fa0dfa16ac45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9d78bd32702c4b678d20827692bf42cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_53212aecc9a94e148325bad9ce32f1fc",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1389353,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1389353,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5c2f54a161d443d8b7cb2cb2d8ddb854"
          }
        },
        "262fb513ad47429581d80f2a47a99424": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_696ba2c824c24adb9f302140f791a95f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.39M/1.39M [00:01&lt;00:00, 1.05MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2e24bcc0cbbf49babb1d7616538db49b"
          }
        },
        "53212aecc9a94e148325bad9ce32f1fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5c2f54a161d443d8b7cb2cb2d8ddb854": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "696ba2c824c24adb9f302140f791a95f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2e24bcc0cbbf49babb1d7616538db49b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/transformers-for-natural-language-processing/blob/main/1-model-architecture-of-the-transformer/3_architecture_of_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E79jgpLzW5Wz"
      },
      "source": [
        "## The Architecture of Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YYdFyPwK5-l"
      },
      "source": [
        "The transformer is one of the most popular state-of-the-art deep learning architectures that is mostly used for natural language processing (NLP) tasks. Ever since the advent of the transformer, it has replaced RNN and LSTM for various tasks. Several new NLP models, such as **BERT, GPT, and T5**, are based on the transformer architecture.\r\n",
        "\r\n",
        "The transformer model is based entirely on the attention mechanism and completely gets rid of recurrence. The transformer uses a special type of attention mechanism called selfattention.\r\n",
        "\r\n",
        "The original Transformer model is a stack of 6 layers. The output of layer $l$ is the input of layer $l+1$ until the final prediction is reached. There is a 6-layer encoder stack on the left and a 6-layer decoder stack on the right:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transformers-for-natural-language-processing/transformer-architecture.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "On the left, the inputs enter the encoder side of the Transformer through an attention sub-layer and FeedForward Network (FFN) sub-layer. \r\n",
        "\r\n",
        "On the right, the target outputs go into the decoder side of the Transformer through two attention sub-layers and an FFN sub-layer. \r\n",
        "\r\n",
        "**We immediately notice that there is no RNN, LSTM, or CNN. Recurrence has been abandoned.**\r\n",
        "\r\n",
        "Attention has replaced recurrence, which requires an increasing number of\r\n",
        "operations as the distance between two words increases. The attention mechanism\r\n",
        "is a \"word-to-word\" operation. The attention mechanism will find how each word\r\n",
        "is related to all other words in a sequence, including the word being analyzed itself.\r\n",
        "\r\n",
        "Let's examine the following sequence:\r\n",
        "\r\n",
        "```\r\n",
        "The cat sat on the mat.\r\n",
        "```\r\n",
        "\r\n",
        "Attention will run dot products between word vectors and determine the strongest\r\n",
        "relationships of a word among all the other words, including itself (\"cat\" and \"cat\"):\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transformers-for-natural-language-processing/attending-words.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "The attention mechanism will provide a deeper relationship between words and\r\n",
        "produce better results.\r\n",
        "\r\n",
        "**For each attention sub-layer, the original Transformer model runs not one but eight attention mechanisms in parallel to speed up the calculations.**\r\n",
        "\r\n",
        "We just looked at the Transformer from the outside. Let's now go into each\r\n",
        "component of the Transformer. We will start with the encoder.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ4E_zMDMe5m"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckyy3zyNMfeB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6ee1486-8517-46d2-917b-caa52e854848"
      },
      "source": [
        "# Transformer Installation\r\n",
        "!pip -qq install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.9MB 9.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 41.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 49.3MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lsyr01GMMh5H"
      },
      "source": [
        "import numpy as np\r\n",
        "from scipy.special import softmax\r\n",
        "from transformers import pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WE-H0Q-4NYM7"
      },
      "source": [
        "## The encoder stack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrjM4m4WNZHT"
      },
      "source": [
        "The layers of the encoder and decoder of the original Transformer model are stacks of layers. Each layer of the encoder stack has the following structure:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transformers-for-natural-language-processing/encoder-stack.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "The original encoder layer structure remains the same for all of the N=6 layers of the Transformer model. Each layer contains two main sub-layers: \r\n",
        "\r\n",
        "- a multi-headed attention mechanism \r\n",
        "- a fully connected position-wise feedforward network\r\n",
        "\r\n",
        "Notice that a residual connection surrounds each main sub-layer, $Sublayer(x)$, in the Transformer model. **These connections transport the unprocessed input $x$ of a sublayer to a layer normalization function. This way, we are certain that key information such as positional encoding is not lost on the way.**\r\n",
        "\r\n",
        "The normalized output of each layer is thus:\r\n",
        "\r\n",
        "$$ LayerNormalization (x + Sublayer(x)) $$\r\n",
        "\r\n",
        "> Though the structure of each of the N=6 layers of the encoder is identical, the content of each layer is not strictly identical to the previous layer.\r\n",
        "\r\n",
        "For example, **the embedding sub-layer is only present at the bottom level of the stack. The other five layers do not contain an embedding layer, and this guarantees that the encoded input is stable through all the layers.**\r\n",
        "\r\n",
        "Also, **the multi-head attention mechanisms perform the same functions from layer 1 to 6. However, they do not perform the same tasks. Each layer learns from the previous layer and explores different ways of associating the tokens in the sequence.**\r\n",
        "It looks for various associations of words, just like how we look for different\r\n",
        "associations of letters and words when we solve a crossword puzzle.\r\n",
        "\r\n",
        "The designers of the Transformer introduced a very efficient constraint. The output of every sub-layer of the model has a constant dimension, including the embedding layer and the residual connections. This dimension is $d_{model}$ and can be set to another value depending on your goals. **In the original Transformer architecture, $d_{model} =512$.**\r\n",
        "\r\n",
        "**$d_{model}$ has a powerful consequence. Practically all the key operations are dot products. The dimensions remain stable, which reduces the number of operations to calculate, reduces machine consumption, and makes it easier to trace the information as it flows through the model.**\r\n",
        "\r\n",
        "This global view of the encoder shows the highly optimized architecture of the\r\n",
        "Transformer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bC3Xb6bBRELp"
      },
      "source": [
        "## Input embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4xZkpBNRFJP"
      },
      "source": [
        "The input embedding sub-layer converts the input tokens to vectors of dimension\r\n",
        "$d_{model} = 512$ using learned embeddings in the original Transformer model. The structure of the input embedding is classical:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transformers-for-natural-language-processing/input-embedding.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "The embedding sub-layer works like other standard transduction models. A\r\n",
        "tokenizer will transform a sentence into tokens. Each tokenizer has its methods,\r\n",
        "but the results are similar.\r\n",
        "\r\n",
        "For example, a tokenizer applied to the sequence \"the Transformer is an innovative NLP model!\" will produce the following tokens in one type of model:\r\n",
        "\r\n",
        "```\r\n",
        "['the', 'transform', 'er', 'is', 'a', 'revolutionary', 'n', 'l', 'p', 'model', '!']\r\n",
        "```\r\n",
        "\r\n",
        "You will notice that this tokenizer normalized the string to lower case and truncated it into subparts. A tokenizer will generally provide an integer representation that will be used for the embedding process.\r\n",
        "\r\n",
        "```\r\n",
        "Text = \"The cat slept on the couch.It was too tired to get up.\"\r\n",
        "\r\n",
        "tokenized text= [1996, 4937, 7771, 2006, 1996, 6411, 1012, 2009, 2001,\r\n",
        "2205, 5458, 2000, 2131, 2039, 1012]\r\n",
        "```\r\n",
        "\r\n",
        "There is not enough information in the tokenized text at this point to go further. The tokenized text must be embedded.\r\n",
        "\r\n",
        "The Transformer contains a learned embedding sub-layer. Many embedding\r\n",
        "methods can be applied to the tokenized input.\r\n",
        "\r\n",
        "A skip-gram will focus on a center word in a window of words and predicts context words. For example, if $word(i)$ is the center word in a two-step window, a skipgram model will analyze $word(i-2), word(i-1), word(i+1)$, and $word(i+2)$. Then the window will slide and repeat the process. A skip-gram model generally contains an input layer, weights, a hidden layer, and an output containing the word embeddings of the tokenized input words.\r\n",
        "\r\n",
        "Suppose we need to perform embedding for the following sentence:\r\n",
        "\r\n",
        "```\r\n",
        "The black cat sat on the couch and the brown dog slept on the rug.\r\n",
        "```\r\n",
        "\r\n",
        "We will focus on two words, black and brown. The word embedding vectors of these\r\n",
        "two words should be similar.\r\n",
        "\r\n",
        "Since we must produce a vector of size $d_{model} = 512$ for each word, we will obtain a size 512 vector embedding for each word:\r\n",
        "\r\n",
        "```\r\n",
        "black=[[-0.01206071 0.11632373 0.06206119 0.01403395 0.09541149\r\n",
        "0.10695464 0.02560172 0.00185677 -0.04284821 0.06146432 0.09466285\r\n",
        "0.04642421 0.08680347 0.05684567 -0.00717266 -0.03163519 0.03292002\r\n",
        "-0.11397766 0.01304929 0.01964396 0.01902409 0.02831945 0.05870414\r\n",
        "0.03390711 -0.06204525 0.06173197 -0.08613958 -0.04654748 0.02728105\r\n",
        "-0.07830904\r\n",
        "…\r\n",
        "0.04340003 -0.13192849 -0.00945092 -0.00835463 -0.06487109 0.05862355\r\n",
        "-0.03407936 -0.00059001 -0.01640179 0.04123065\r\n",
        "-0.04756588 0.08812257 0.00200338 -0.0931043 -0.03507337 0.02153351\r\n",
        "-0.02621627 -0.02492662 -0.05771535 -0.01164199\r\n",
        "-0.03879078 -0.05506947 0.01693138 -0.04124579 -0.03779858\r\n",
        "-0.01950983 -0.05398201 0.07582296 0.00038318 -0.04639162\r\n",
        "-0.06819214 0.01366171 0.01411388 0.00853774 0.02183574\r\n",
        "-0.03016279 -0.03184025 -0.04273562]]\r\n",
        "```\r\n",
        "\r\n",
        "The word black is now represented by 512 dimensions. Other embedding methods\r\n",
        "could be used and $d_{model}$ could have a higher number of dimensions.\r\n",
        "\r\n",
        "The word embedding of brown is also represented by 512 dimensions:\r\n",
        "\r\n",
        "```\r\n",
        "brown=[[ 1.35794589e-02 -2.18823571e-02 1.34526128e-02 6.74355254e-02\r\n",
        "1.04376070e-01 1.09921647e-02 -5.46298288e-02 -1.18385479e-02\r\n",
        "4.41223830e-02 -1.84863899e-02 -6.84073642e-02 3.21860164e-02\r\n",
        "4.09143828e-02 -2.74433400e-02 -2.47369967e-02 7.74542615e-02\r\n",
        "9.80964210e-03 2.94299088e-02 2.93895267e-02 -3.29437815e-02\r\n",
        "…\r\n",
        "7.20389187e-02 1.57317147e-02 -3.10291946e-02 -5.51304631e-02\r\n",
        "-7.03861639e-02 7.40829483e-02 1.04319192e-02 -2.01565702e-03\r\n",
        "2.43322570e-02 1.92969330e-02 2.57341694e-02 -1.13280728e-01\r\n",
        "8.45847875e-02 4.90090018e-03 5.33546880e-02 -2.31553353e-02\r\n",
        "3.87288055e-05 3.31782512e-02 -4.00604047e-02 -1.02028981e-01\r\n",
        "3.49597558e-02 -1.71501152e-02 3.55573371e-02 -1.77437533e-02\r\n",
        "-5.94457164e-02 2.21221056e-02 9.73121971e-02 -4.90022525e-02]]\r\n",
        "```\r\n",
        "\r\n",
        "**To verify the word embedding produced for these two words, we can use cosine\r\n",
        "similarity to see if the word embeddings of the words black and brown are similar.**\r\n",
        "\r\n",
        "Cosine similarity uses Euclidean (L2) norm to create vectors in a unit sphere. The dot product of the vectors we are comparing is the cosine between the points of those two vectors.\r\n",
        "\r\n",
        "The cosine similarity between the black vector of size $d_{model} = 512$ and brown vector of size $d_{model} = 512$ in the embedding of the example is:\r\n",
        "\r\n",
        "```\r\n",
        "cosine_similarity(black, brown)= [[0.9998901]]\r\n",
        "```\r\n",
        "\r\n",
        "The skip-gram produced two vectors that are very close to each other. It detected that `black` and `brown` form a color subset of the dictionary of words.\r\n",
        "\r\n",
        "The Transformer's subsequent layers do not start empty-handed. They have learned\r\n",
        "word embeddings that already provide information on how the words can be\r\n",
        "associated.\r\n",
        "\r\n",
        "**However, a big chunk of information is missing because no additional vector or\r\n",
        "information indicates a word's position in a sequence.**\r\n",
        "\r\n",
        "The designers of the Transformer came up with yet another innovative feature:\r\n",
        "**positional encoding**.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZanAOfKqjx3f"
      },
      "source": [
        "## Positional encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwnxMCE5j0ay"
      },
      "source": [
        "We enter this positional encoding function of the Transformer with no idea of the position of a word in a sequence:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transformers-for-natural-language-processing/position-encoding.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "We cannot create independent positional vectors that would have a high cost on the training speed of the Transformer and make attention sub-layers very complex to work with. The idea is to add a positional encoding value to the input embedding instead of having additional vectors to describe the position of a token in a sequence.\r\n",
        "\r\n",
        "Please refer this notebook for  [positional encoding](https://github.com/rahiakela/transformers-for-natural-language-processing/blob/main/1-model-architecture-of-the-transformer/1_positional_encoding.ipynb).\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGUA2tS6MXyW"
      },
      "source": [
        "## Sub-layer 1: Multi-head attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hvi8cwmPW6dv"
      },
      "source": [
        "**The multi-head attention sub-layer contains eight heads and is followed by postlayer normalization, which will add residual connections to the output of the sublayer and normalize it.**\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transformers-for-natural-language-processing/sub-layer-1.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "The input of the multi-attention sub-layer of the first layer of the encoder stack is a vector that contains the embedding and the positional encoding of each word. The next layers of the stack do not start these operations over.\r\n",
        "\r\n",
        "The dimension of the vector of each word $x_n$ of an input sequence is $d_{model} = 512$:\r\n",
        "\r\n",
        "$$\r\n",
        "pe(x_n) = [d_1=9.09297407e-01, d_2=9.09297407e-01, .., d_{512}=1.00000000e+00]\r\n",
        "$$\r\n",
        "\r\n",
        "The representation of each word $x_n$ has become a vector of $d_{model} = 512$ dimensions.\r\n",
        "\r\n",
        "**Each word is mapped to all the other words to determine how it fits in a sequence.**\r\n",
        "\r\n",
        "In the following sentence, we can see that \"it\" could be related to \"cat\" and \"rug\" in the sequence:\r\n",
        "\r\n",
        "```\r\n",
        "Sequence =The cat sat on the rug and it was dry-cleaned.\r\n",
        "```\r\n",
        "\r\n",
        "**The model will train to find out if \"it\" is related to \"cat\" or \"rug.\"** We could run a huge calculation by training the model using the $d_{model} = 512$ dimensions as they are now.\r\n",
        "\r\n",
        "However, we would only get one point of view at a time by analyzing the sequence\r\n",
        "with one $d_{model}$ block. Furthermore, it would take quite some calculation time to find other perspectives.\r\n",
        "\r\n",
        "**A better way is to divide the $d_{model} = 512$ dimensions of each word $x_n$ of $x$ (all of the words of a sequence) into $8 d_k = 64$ dimensions.**\r\n",
        "\r\n",
        "**We then can run the 8 \"heads\" in parallel to speed up the training and obtain 8 different representation subspaces of how each word relates to another:**\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transformers-for-natural-language-processing/multi-head-representations.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "**You can see that there are now 8 heads running in parallel.** One head might decide that \"it\" fits well with \"cat\" and another that \"it\" fits well with \"rug\" and another that \"rug\" fits well with \"dry-cleaned.\"\r\n",
        "\r\n",
        "The output of each head is a matrix $z_i$ with a shape of $x^*d_k$ The output of a multiattention head is $Z$ defined as:\r\n",
        "\r\n",
        "$$ Z = (z_0, z_1, z_2, z_3, z_4, z_5, z_6, z_7,) $$\r\n",
        "\r\n",
        "**However, $Z$ must be concatenated so that the output of the multi-head sub-layer is not a sequence of dimensions but one lines of $xm*d_{model}$ matrix.**\r\n",
        "\r\n",
        "Before exiting the multi-head attention sub-layer, the elements of $Z$ are concatenated:\r\n",
        "\r\n",
        "$$ MultiHead(output) = Concat(z_0, z_1, z_2, z_3, z_4, z_5, z_6, z_7,) = x, d_{model} $$\r\n",
        "\r\n",
        "**Notice that each head is concatenated into $z$ that has a dimension of $d_{model} = 512$. The output of the multi-headed layer respects the constraint of the original Transformer model.**\r\n",
        "\r\n",
        "Inside each head $h_n$ of the attention mechanism, each word vector has three\r\n",
        "representations:\r\n",
        "\r\n",
        "- A query vector $(Q)$ that has a dimension of $d_q = 64$, which is activated and trained when a word vector $x_n$ seeks all of the key-value pairs of the other word vectors, including itself in self-attention\r\n",
        "- A key vector $(K)$ that has a dimension of $d_k = 64$, which will be trained to provide an attention value\r\n",
        "- A value vector $(V)$ that has a dimension of $d_v = 64$, which will be trained to provide another attention value\r\n",
        "\r\n",
        "\r\n",
        "Attention is defined as **Scaled Dot-Product Attention** which is represented in the following equation in which we plug $Q$, $K$ and $V$:\r\n",
        "\r\n",
        "$$\r\n",
        "Attention(Q,K,V) = softmax \\begin{pmatrix} \\frac{QK^T}{\\sqrt{d_k}} \\end{pmatrix} V\r\n",
        "$$\r\n",
        "\r\n",
        "**The vectors all have the same dimension making it relatively simple to use a scaled dot product to obtain the attention values for each head and then concatenate the output Z of the 8 heads.**\r\n",
        "\r\n",
        "To obtain $Q$, $K$, and $V$, we must train the model with their respective weight matrices $Q_w, K_w$ and $V_w$, which have $d_k = 64$ columns and $d_{model} = 512$ rows. For example, $Q$ is obtained by a dot-product between $x$ and $Q_w. Q$ will have a dimension of $d_k = 64$.\r\n",
        "\r\n",
        "Hugging Face and Google Brain Trax, among others, provide ready-to-use\r\n",
        "frameworks, libraries, and modules. However, let's open the hood of the Transformer model and get our hands dirty in Python to illustrate the architecture we just explored in order to visualize the model in code and show it with intermediate images.\r\n",
        "\r\n",
        "We will use basic Python code with only numpy and a softmax function in 10 steps to run the key aspects of the attention mechanism.\r\n",
        "\r\n",
        "We will start by only using minimal Python functions to understand the Transformer at a low level with the inner workings of an attention head. We will explore the inner workings of the multi-head attention sub-layer using basic code.\r\n",
        "\r\n",
        "\r\n",
        "Please refer this notebook for [implemetation of Multi-head attention](https://github.com/rahiakela/transformers-for-natural-language-processing/blob/main/1-model-architecture-of-the-transformer/2_architecture_of_multi_head_attention.ipynb).\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfEcGd70LHkE"
      },
      "source": [
        "## Post-layer normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKzsFk7sLIeg"
      },
      "source": [
        "Each attention sub-layer and each feedforward sub-layer of the Transformer is\r\n",
        "followed by post-layer normalization (Post-LN):\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transformers-for-natural-language-processing/post-layer-normalization.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "**The Post-LN contains an add function and a layer normalization process. The add function processes the residual connections that come from the input of the sublayer. The goal of the residual connections is to make sure critical information is not lost.**\r\n",
        "\r\n",
        "$$ LayerNorm(x+Sublayer(x)) $$\r\n",
        "\r\n",
        "$Sublayer(x)$ is the sub-layer itself. $x$ is the information available at the input step of $Sublayer(x)$.\r\n",
        "\r\n",
        "The input of $LayerNorm$ is a vector $v$ resulting from $x + Sublayer(x)$. $d_{model} = 512$ for every input and output of the Transformer, which standardizes all the processes.\r\n",
        "\r\n",
        "Many layer normalization methods exist, and variations exist from one model to\r\n",
        "another. The basic concept for $v= x + Sublayer(x)$ can be defined by $LayerNorm(v)$:\r\n",
        "\r\n",
        "$$ LayerNorm(v)=\\gamma \\frac{v - \\mu}{\\sigma} + \\beta $$\r\n",
        "\r\n",
        "The variables are: \r\n",
        "\r\n",
        "- $\\mu$ is the mean of $v$ of dimension $d$. As such:\r\n",
        "\r\n",
        "$$ \\mu = \\frac{1}{d}\\sum_{k=1}^{d}v_k $$\r\n",
        "\r\n",
        "- $\\sigma$ is the standard deviation $v$ of dimension $d$. As such:\r\n",
        "\r\n",
        "$$ \\sigma^2 = \\frac{1}{d}\\sum_{k=1}^{d} (v_{k-\\mu})^2 $$\r\n",
        "\r\n",
        "- $\\gamma$ is a scaling parameter.\r\n",
        "\r\n",
        "- $\\beta$ is a bias vector.\r\n",
        "\r\n",
        "This version of $LayerNorm(v)$ shows the general idea of the many possible Post-LN methods.\r\n",
        "\r\n",
        "The next sub-layer can now process the output of the Post-LN or $LayerNorm(v)$. In this case, the sub-layer is a feedforward network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15wunLV8nRlG"
      },
      "source": [
        "## Sub-layer 2: Feedforward network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAGkoRqRnUDe"
      },
      "source": [
        "The input of the FFN is the $d_{model} = 512$ output of the Post-LN of the previous sublayer:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transformers-for-natural-language-processing/feedforward-sub-layer.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "The FFN sub-layer can be described as follows:\r\n",
        "\r\n",
        "- The FFNs in the encoder and decoder are fully connected.\r\n",
        "- The FFN is a position-wise network. Each position is processed separately\r\n",
        "and in an identical way.\r\n",
        "- The FFN contains two layers and applies a ReLU activation function.\r\n",
        "- The input and output of the FFN layers is $d_{model} = 512$, but the inner layer is larger with $d_{ff} = 2048$.\r\n",
        "- The FFN can be viewed as performing two kernel size 1 convolutions.\r\n",
        "\r\n",
        "Taking this description into account, we can describe the optimized and\r\n",
        "standardized FFN as follows:\r\n",
        "\r\n",
        "$$ FFN(x) = max(0, xW_1 + b_1)W_2 =b_2 $$\r\n",
        "\r\n",
        "**The output of the FFN goes to the Post-LN, as described in the previous section. Then the output is sent to the next layer of the encoder stack and the multi-head attention layer of the decoder stack.**\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOHdt7MblRyY"
      },
      "source": [
        "## The decoder stack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk-kCJXSlSyN"
      },
      "source": [
        "The layers of the decoder of the Transformer model are stacks of layers like the encoder layers. Each layer of the decoder stack has the following structure:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transformers-for-natural-language-processing/decoder-stack.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "The structure of the decoder layer remains the same as the encoder for all the N=6 layers of the Transformer model. Each layer contains three sub-layers: \r\n",
        "\r\n",
        "- a multiheaded masked attention mechanism \r\n",
        "- a multi-headed attention mechanism\r\n",
        "- a fully connected position-wise feedforward network\r\n",
        "\r\n",
        "**The decoder has a third main sub-layer, which is the masked multi-head attention mechanism. In this sub-layer output, at a given position, the following words are masked so that the Transformer bases its assumptions on its inferences without seeing the rest of the sequence. That way, in this model, it cannot see future parts of the sequence.**\r\n",
        "\r\n",
        "A residual connection, $Sublayer(x)$, surrounds each of the three main sub-layers in the Transformer model like in the encoder stack:\r\n",
        "\r\n",
        "$$ LayerNormalization(x + Sublayer(x)) $$\r\n",
        "\r\n",
        "The embedding layer sub-layer is only present at the bottom level of the stack, like for the encoder stack. The output of every sub-layer of the decoder stack has a constant dimension, $d_{model}$ like in the encoder stack, including the embedding layer and the output of the residual connections.\r\n",
        "\r\n",
        "**The structure of each sub-layer and function of the decoder is similar to the encoder. So, we can refer to the encoder for the same functionality when we need to. We will only focus on the differences between the decoder and the encoder.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0FhpvXxtOqL"
      },
      "source": [
        "## Output embedding and position encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq6n47wStP3e"
      },
      "source": [
        "The structure of the sub-layers of the decoder is mostly the same as the sub-layers of the encoder. The output embedding layer and position encoding function are the same as in the encoder stack.\r\n",
        "\r\n",
        "In the Transformer usage we are exploring through the model, the output is a translation we need to learn. I chose to use a French translation:\r\n",
        "\r\n",
        "```\r\n",
        "Output=Le chat noir était assis sur le canapé et le chien marron\r\n",
        "dormait sur le tapis\r\n",
        "```\r\n",
        "\r\n",
        "This output is the French translation of the English input sentence:\r\n",
        "\r\n",
        "```\r\n",
        "Input=The black cat sat on the couch and the brown dog slept on the\r\n",
        "rug.\r\n",
        "```\r\n",
        "\r\n",
        "The output words go through the word embedding layer, and then the positional\r\n",
        "encoding function, like in the first layer of the encoder stack."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clqp66AxuF7o"
      },
      "source": [
        "## The attention layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BGp0RMbuG5k"
      },
      "source": [
        "**The Transformer is an auto-regressive model. It uses the previous output sequences as an additional input. The multi-head attention layers of the decoder use the same process as the encoder.**\r\n",
        "\r\n",
        "However, the masked multi-head attention sub-layer 1 only lets attention apply to the positions up to and including the current position. The future words are hidden from the Transformer, and this forces it to learn how to predict.\r\n",
        "\r\n",
        "A post-layer normalization process follows the masked multi-head attention sublayer 1 as in the encoder.\r\n",
        "\r\n",
        "The multi-head attention sub-layer 2 also only attends to the positions up to the current position the Transformer is predicting to avoid seeing the sequence it must predict.\r\n",
        "\r\n",
        "The multi-head attention sub-layer 2 draws information from the encoder by taking $encoder (K, V)$ into account during the dot-product attention operations. This sublayer also draws information from the masked multi-head attention sub-layer 1 (masked attention) by also taking $sub-layer 1(Q)$ into account during the dot-product attention operations. The decoder thus uses the trained information of the encoder.\r\n",
        "\r\n",
        "We can define the input of the self-attention multi-head sub-layer of a decoder as:\r\n",
        "\r\n",
        "```\r\n",
        "Input_Attention=(Output_decoder_sub_layer-1(Q), Output_encoder_layer(K,V))\r\n",
        "```\r\n",
        "\r\n",
        "A post-layer normalization process follows the masked multi-head attention sub-layer 1 as in the encoder.\r\n",
        "\r\n",
        "The Transformer then goes to the FFN sub-layer, followed by a Post-LN and the\r\n",
        "linear layer.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7joGNZSwJWR"
      },
      "source": [
        "## The FFN sub-layer, the Post-LN, and the linear layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbV_2bUrwRlp"
      },
      "source": [
        "The FFN sub-layer has the same structure as the FFN of the encoder stack. The Post-LN of the FFN works as the layer normalization of the encoder stack.\r\n",
        "\r\n",
        "The Transformer produces an output sequence of only one element at a time:\r\n",
        "\r\n",
        "$$ OutputSequence= (y_1, y_2, … y_n) $$\r\n",
        "\r\n",
        "The linear layer produces an output sequence with a linear function that varies per model but relies on the standard method:\r\n",
        "\r\n",
        "$$ y = w * x + b $$\r\n",
        "\r\n",
        "$x$ and $b$ are learned parameters.\r\n",
        "\r\n",
        "**The linear layer will thus produce the next probable elements of a sequence that a softmax function will convert into a probable element.**\r\n",
        "\r\n",
        "The decoder layer as the encoder layer will then go from layer $l$ to layer $l+1$ up to the top layer of the N=6-layer transformer stack."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZR9tV_fyskE"
      },
      "source": [
        "## Transformer in Action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6iXvISsyvZt"
      },
      "source": [
        "The original Transformer was trained on a 4.5-million-sentence-pair English-German dataset and a 36-million-sentence English-French dataset.\r\n",
        "\r\n",
        "The training of the original Transformer base models took 12 hours to train for 100,000 steps on a machine with 8 NVIDIA P100 GPUs. The big models took 3.5 days for 300,000 steps.\r\n",
        "\r\n",
        "The original Transformer outperformed all the previous machine translation models with a BLEU score of 41.8. The result was obtained on the WMT English-to-French dataset.\r\n",
        "\r\n",
        "With Hugging Face, you can implement machine translation in three lines of code!\r\n",
        "\r\n",
        "We implement the Hugging Face pipeline that contains several transformer\r\n",
        "usages. The pipeline contains ready-to-use functions. In our case, to illustrate the Transformer model, we activate the translator model and enter a\r\n",
        "sentence to translate from English to French:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232,
          "referenced_widgets": [
            "50780f87f73a482884930e38f48a048b",
            "a9c524dc924d4ceeb72cdc879f6a5e57",
            "92e469deaaad453dab41aba22b7a29c4",
            "61c87acb992a4d7087c7d84ca5dd280e",
            "3f1fbb70b63445828f4858ec8683ed26",
            "516301f1d6084408b2dd492ba85eda4a",
            "a805f3c5aaa549459dd33650f1c354d9",
            "09c4b3c66b8349bd88ab9c3018488191",
            "485caf29bba544c7b40a8a2cc67b63fa",
            "e357e98bb02d45f284b6b4715557c3f4",
            "e2b249311835466c940ed323960cdf50",
            "9397616eda7a4b4390c1e381fe4149cb",
            "f7a3169e2ac24c578bb04bd4d4384a48",
            "b1a007348cfe4c6fbbf3c2bbd7a6cea2",
            "bdda70c36128462286650bad783efe5d",
            "5571d04e945d4b35a20b2af24a5368de",
            "77bbfb1084ef42789cc12744d7ad568e",
            "036450c1dde64ac5927293712c8c5194",
            "3bbe1781263548f0b3c2538d39280434",
            "0b4e804209744998802e182f93315d58",
            "47cb867dbbf04b1a94cfdedeacc1a6af",
            "5fe8b25b627948fd8a3e67ef8a9a5af3",
            "4d7e2853f08b442ca76a820d28f23080",
            "0437c9aa007c4ba4a72b9479df8f1ca7",
            "33603decb5a949b0ba6561ed085c9517",
            "58212e94fa634e7db3f8fa0dfa16ac45",
            "9d78bd32702c4b678d20827692bf42cb",
            "262fb513ad47429581d80f2a47a99424",
            "53212aecc9a94e148325bad9ce32f1fc",
            "5c2f54a161d443d8b7cb2cb2d8ddb854",
            "696ba2c824c24adb9f302140f791a95f",
            "2e24bcc0cbbf49babb1d7616538db49b"
          ]
        },
        "id": "dqIdaINCzdFd",
        "outputId": "141af5d8-8c3b-4a57-b9fe-2a250cef64f8"
      },
      "source": [
        "translator = pipeline(\"translation_en_to_fr\")\r\n",
        "\r\n",
        "# One line of code!\r\n",
        "print(translator(\"It is easy to translate languages with transformers\", max_length=40))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "50780f87f73a482884930e38f48a048b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1199.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "485caf29bba544c7b40a8a2cc67b63fa",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=891691430.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77bbfb1084ef42789cc12744d7ad568e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=791656.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "33603decb5a949b0ba6561ed085c9517",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1389353.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[{'translation_text': \"Il est facile de traduire des langues à l'aide de transformateurs\"}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkKcKJmV0GB9"
      },
      "source": [
        "And voilà! The translation is displayed.\r\n",
        "\r\n",
        "Hugging Face shows how transformer architectures can be used in ready-to-use\r\n",
        "models.\r\n",
        "\r\n",
        "The arrival of the Transformer marks the beginning of a new generation of ready-to-use artificial intelligence models. For example, Hugging Face and Google Brain make artificial intelligence easy to implement with a few lines of code."
      ]
    }
  ]
}