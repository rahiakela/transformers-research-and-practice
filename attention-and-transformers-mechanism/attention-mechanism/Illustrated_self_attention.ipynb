{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Illustrated-self-attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/natural-language-processing-case-studies/blob/bert-transformer-labs/Illustrated_self_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XQ6NsIuDtgr"
      },
      "source": [
        "## Illustrated: Self-Attention\n",
        "Step-by-step guide to self-attention with illustrations and code\n",
        "\n",
        "[Illustrated: Self-Attention](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a)\n",
        "\n",
        "[Raimi Karim](https://towardsdatascience.com/@remykarem)\n",
        "\n",
        "> Colab made by: [Manuel Romero](https://twitter.com/mrm8488)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U76qWlrbOmx7"
      },
      "source": [
        "![texto alternativo](https://miro.medium.com/max/1973/1*_92bnsMJy8Bl539G4v93yg.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOkXKd60Q_Iu"
      },
      "source": [
        "What do *BERT, RoBERTa, ALBERT, SpanBERT, DistilBERT, SesameBERT, SemBERT, MobileBERT, TinyBERT and CamemBERT* all have in common? And Iâ€™m not looking for the answer â€œBERTâ€ ðŸ¤­.\n",
        "\n",
        "Answer: **self-attention** ðŸ¤—. We are not only talking about architectures bearing the name â€œBERTâ€™, but more correctly **Transformer-based architectures**. Transformer-based architectures, which are primarily used in modelling language understanding tasks, eschew the use of recurrence in neural network (RNNs) and instead trust entirely on self-attention mechanisms to draw global dependencies between inputs and outputs. But whatâ€™s the math behind this?\n",
        "\n",
        "Thatâ€™s what weâ€™re going to find out today. The main content of this post is to walk you through the mathematical operations involved in a self-attention module. By the end of this article, you should be able to write or code a self-attention module from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atUYzU3TSD9z"
      },
      "source": [
        "## Step 0. What is self-attention?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvO9-72GRjYl"
      },
      "source": [
        "If youâ€™re thinking if self-attention is similar to attention, then the answer is yes! They fundamentally share the same concept and many common mathematical operations.\r\n",
        "A self-attention module takes in n inputs, and returns n outputs. What happens in this module? In laymanâ€™s terms, the self-attention mechanism allows the inputs to interact with each other (â€œselfâ€) and find out who they should pay more attention to (â€œattentionâ€). The outputs are aggregates of these interactions and attention scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDMmHAaSTE6P"
      },
      "source": [
        "The illustrations are divided into the following steps:\n",
        "1. Prepare inputs\n",
        "2. Initialise weights\n",
        "3. Derive key, query and value\n",
        "4. Calculate attention scores for Input 1\n",
        "5. Calculate softmax\n",
        "6. Multiply scores with values\n",
        "7. Sum weighted values to get Output 1\n",
        "8. Repeat steps 4â€“7 for Input 2 & Input 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1UxPJlHBVmS"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENdzUZqSBsiB"
      },
      "source": [
        "## Step 1: Prepare inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DY0aMZuIRnE2"
      },
      "source": [
        "For this tutorial, for the shake of simplicity, we start with 3 inputs, each with dimension 4.\r\n",
        "\r\n",
        "![texto alternativo](https://miro.medium.com/max/1973/1*hmvdDXrxhJsGhOQClQdkBA.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKYrJsljBhnv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9a2c81f-665a-448b-ff7f-83cf6f853ad5"
      },
      "source": [
        "x = [\n",
        "  [1, 0, 1, 0], # Input 1\n",
        "  [0, 2, 0, 2], # Input 2\n",
        "  [1, 1, 1, 1]  # Input 3\n",
        "]\n",
        "\n",
        "x = torch.tensor(x, dtype=torch.float32)\n",
        "x"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 1., 0.],\n",
              "        [0., 2., 0., 2.],\n",
              "        [1., 1., 1., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ96EoE1Bvat"
      },
      "source": [
        "## Step 2: Initialise weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mrc-foE_SJjy"
      },
      "source": [
        "Every input must have three representations (see diagram below). These representations are called **key** (orange), **query** (red), and **value** (purple). For this example, letâ€™s take that we want these representations to have a dimension of 3. Because every input has a dimension of 4, this means each set of the weights must have a shape of 4Ã—3.\r\n",
        "\r\n",
        "![texto del enlace](https://miro.medium.com/max/1975/1*VPvXYMGjv0kRuoYqgFvCag.gif)\r\n",
        "\r\n",
        ">**Note**\r\n",
        "Weâ€™ll see later that the dimension of value is also the dimension of the output.\r\n",
        "\r\n",
        "\r\n",
        "In order to obtain these representations, every input (green) is multiplied with a set of weights for keys, a set of weights for querys (I know thatâ€™s not the right spelling), and a set of weights for values. In our example, we initialise the three sets of weights as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUTNr15JBkSG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "092dabd0-c688-4c45-990b-f8fb5fe1ae83"
      },
      "source": [
        "# Weights for key\n",
        "w_key = [\n",
        "  [0, 0, 1],\n",
        "  [1, 1, 0],\n",
        "  [0, 1, 0],\n",
        "  [1, 1, 0]\n",
        "]\n",
        "\n",
        "# Weights for query\n",
        "w_query = [\n",
        "  [1, 0, 1],\n",
        "  [1, 0, 0],\n",
        "  [0, 0, 1],\n",
        "  [0, 1, 1]\n",
        "]\n",
        "\n",
        "# Weights for value\n",
        "w_value = [\n",
        "  [0, 2, 0],\n",
        "  [0, 3, 0],\n",
        "  [1, 0, 3],\n",
        "  [1, 1, 0]\n",
        "]\n",
        "\n",
        "w_key = torch.tensor(w_key, dtype=torch.float32)\n",
        "w_query = torch.tensor(w_query, dtype=torch.float32)\n",
        "w_value = torch.tensor(w_value, dtype=torch.float32)\n",
        "\n",
        "print(\"Weights for key: \\n\", w_key)\n",
        "print(\"Weights for query: \\n\", w_query)\n",
        "print(\"Weights for value: \\n\", w_value)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights for key: \n",
            " tensor([[0., 0., 1.],\n",
            "        [1., 1., 0.],\n",
            "        [0., 1., 0.],\n",
            "        [1., 1., 0.]])\n",
            "Weights for query: \n",
            " tensor([[1., 0., 1.],\n",
            "        [1., 0., 0.],\n",
            "        [0., 0., 1.],\n",
            "        [0., 1., 1.]])\n",
            "Weights for value: \n",
            " tensor([[0., 2., 0.],\n",
            "        [0., 3., 0.],\n",
            "        [1., 0., 3.],\n",
            "        [1., 1., 0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pr9XZF9X_Ed"
      },
      "source": [
        ">**Note**: *In a neural network setting, these weights are usually small numbers, initialised randomly using an appropriate random distribution like Gaussian, Xavier and Kaiming distributions.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxGT5awVB1Xw"
      },
      "source": [
        "## Step 3: Derive key, query and value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzpdW3_ogvDd"
      },
      "source": [
        "Now that we have the three sets of weights, letâ€™s actually obtain the **key**, **query** and **value** representations for every input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ1oQTREgee0"
      },
      "source": [
        "### Obtaining the keys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huRLFzqodzbV"
      },
      "source": [
        "\r\n",
        "Key representation for Input 1:\r\n",
        "\r\n",
        "```\r\n",
        "               [0, 0, 1]\r\n",
        "[1, 0, 1, 0] x [1, 1, 0] = [0, 1, 1]\r\n",
        "               [0, 1, 0]\r\n",
        "               [1, 1, 0]\r\n",
        "\r\n",
        "               (1x4)(4x3)    = (1x3)\r\n",
        "input vector x weight metrix = key vector\r\n",
        "```\r\n",
        "\r\n",
        "Use the same set of weights to get the key representation for Input 2:\r\n",
        "\r\n",
        "```\r\n",
        "               [0, 0, 1]\r\n",
        "[0, 2, 0, 2] x [1, 1, 0] = [4, 4, 0]\r\n",
        "               [0, 1, 0]\r\n",
        "               [1, 1, 0]\r\n",
        "          \r\n",
        "```\r\n",
        "\r\n",
        "Use the same set of weights to get the key representation for Input 3:\r\n",
        "\r\n",
        "```\r\n",
        "               [0, 0, 1]\r\n",
        "[1, 1, 1, 1] x [1, 1, 0] = [2, 3, 1]\r\n",
        "               [0, 1, 0]\r\n",
        "               [1, 1, 0]\r\n",
        "```\r\n",
        "\r\n",
        "A faster way is to vectorise the above operations:\r\n",
        "\r\n",
        "```\r\n",
        "               [0, 0, 1]\r\n",
        "[1, 0, 1, 0]   [1, 1, 0]   [0, 1, 1]\r\n",
        "[0, 2, 0, 2] x [0, 1, 0] = [4, 4, 0]\r\n",
        "[1, 1, 1, 1]   [1, 1, 0]   [2, 3, 1]\r\n",
        "\r\n",
        "           (3x4)(4x3)    = (3x3)\r\n",
        "input metrix x weight metrix = key metrix\r\n",
        "```\r\n",
        "![texto alternativo](https://miro.medium.com/max/1975/1*dr6NIaTfTxEWzxB2rc0JWg.gif)\r\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQwhDIi7aGXp"
      },
      "source": [
        "### Obtaining the values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi0EblXTamFz"
      },
      "source": [
        "Letâ€™s do the same to obtain the value representations for every input:\n",
        "\n",
        "```\n",
        "               [0, 2, 0]\n",
        "[1, 0, 1, 0]   [0, 3, 0]   [1, 2, 3] \n",
        "[0, 2, 0, 2] x [1, 0, 3] = [2, 8, 0]\n",
        "[1, 1, 1, 1]   [1, 1, 0]   [2, 6, 3]\n",
        "\n",
        "           (3x4)(4x3)    = (3x3)\n",
        "input metrix x weight metrix = value metrix\n",
        "```\n",
        "\n",
        "![texto alternativo](https://miro.medium.com/max/1975/1*5kqW7yEwvcC0tjDOW3Ia-A.gif)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m56d4GmPi6RS"
      },
      "source": [
        "### Obtaining the querys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTp2izu1bLNq"
      },
      "source": [
        "and finally the query representations:\n",
        "\n",
        "```\n",
        "               [1, 0, 1]\n",
        "[1, 0, 1, 0]   [1, 0, 0]   [1, 0, 2]\n",
        "[0, 2, 0, 2] x [0, 0, 1] = [2, 2, 2]\n",
        "[1, 1, 1, 1]   [0, 1, 1]   [2, 1, 3]\n",
        "\n",
        "           (3x4)(4x3)    = (3x3)\n",
        "input metrix x weight metrix = query metrix\n",
        "```\n",
        "\n",
        "![texto alternativo](https://miro.medium.com/max/1975/1*wO_UqfkWkv3WmGQVHvrMJw.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j85hpuBwk-bb"
      },
      "source": [
        "### Calculating key, value and query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFAbbmM2lKj8"
      },
      "source": [
        ">**Notes**: \r\n",
        "In practice, a bias vector may be added to the product of matrix multiplication."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rv2NXynOB7oG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "028f35ed-d4ae-42b2-d836-42a34ed3858d"
      },
      "source": [
        "keys = x @ w_key\n",
        "querys = x @ w_query\n",
        "values = x @ w_value\n",
        "\n",
        "print(\"Keys: \\n\", keys)\n",
        "# tensor([[0., 1., 1.],\n",
        "#         [4., 4., 0.],\n",
        "#         [2., 3., 1.]])\n",
        "\n",
        "print(\"Querys: \\n\", querys)\n",
        "# tensor([[1., 0., 2.],\n",
        "#         [2., 2., 2.],\n",
        "#         [2., 1., 3.]])\n",
        "print(\"Values: \\n\", values)\n",
        "# tensor([[1., 2., 3.],\n",
        "#         [2., 8., 0.],\n",
        "#         [2., 6., 3.]])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Keys: \n",
            " tensor([[0., 1., 1.],\n",
            "        [4., 4., 0.],\n",
            "        [2., 3., 1.]])\n",
            "Querys: \n",
            " tensor([[1., 0., 2.],\n",
            "        [2., 2., 2.],\n",
            "        [2., 1., 3.]])\n",
            "Values: \n",
            " tensor([[1., 2., 3.],\n",
            "        [2., 8., 0.],\n",
            "        [2., 6., 3.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pmf0OQhCnD8"
      },
      "source": [
        "## Step 4: Calculate attention scores\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilTUG-aFllUh"
      },
      "source": [
        "![texto alternativo](https://miro.medium.com/max/1973/1*u27nhUppoWYIGkRDmYFN2A.gif)\r\n",
        "\r\n",
        "To obtain **attention scores**, we start off with taking a dot product between Input 1â€™s **query** (red) with **all keys** (orange), including itself. Since there are 3 key representations (because we have 3 inputs), we obtain 3 attention scores (blue).\r\n",
        "\r\n",
        "```\r\n",
        "            [0, 4, 2]\r\n",
        "[1, 0, 2] x [1, 4, 3] = [2, 4, 4]\r\n",
        "            [1, 0, 1]\r\n",
        "\r\n",
        "         (1x3)(3x3)   = (1x3)\r\n",
        "query vector x transpose(key metrix)  = attention score\r\n",
        "```\r\n",
        "\r\n",
        "Notice that we only use the **query** from Input 1. Later weâ€™ll work on repeating this same step for the other **querys**.\r\n",
        "\r\n",
        ">**Note**: *The above operation is known as dot product attention, one of the several [score functions](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3). Other score functions include scaled dot product and additive/concat.*   \r\n",
        "\r\n",
        "Let's calculate it for all queries\r\n",
        "```\r\n",
        "[1, 0, 2]   [0, 4, 2]   [2,  4,  4]\r\n",
        "[2, 2, 2] x [1, 4, 3] = [4, 16, 12]\r\n",
        "[2, 1, 3]   [1, 0, 1]   [4, 12, 10]\r\n",
        "\r\n",
        "\r\n",
        "         (3x3)(3x3)   = (3x3)\r\n",
        "query metrix x transpose(key metrix)  = attention score metrix\r\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GDhKEl0Cokw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3445187-e43c-4ca4-8ce8-48b0239f619a"
      },
      "source": [
        "attn_scores = querys @ keys.T\n",
        "print(attn_scores)\n",
        "\n",
        "# tensor([[ 2.,  4.,  4.],  # attention scores from Query 1\n",
        "#         [ 4., 16., 12.],  # attention scores from Query 2\n",
        "#         [ 4., 12., 10.]]) # attention scores from Query 3"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 2.,  4.,  4.],\n",
            "        [ 4., 16., 12.],\n",
            "        [ 4., 12., 10.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO3NmnbvCxpX"
      },
      "source": [
        "## Step 5: Calculate softmax\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb1uQrZLqJEM"
      },
      "source": [
        "![texto alternativo](https://miro.medium.com/max/1973/1*jf__2D8RNCzefwS0TP1Kyg.gif)\r\n",
        "\r\n",
        "Take the **[softmax](https://en.wikipedia.org/wiki/Softmax_function)** across these **attention scores** (blue).\r\n",
        "```\r\n",
        "softmax([2, 4, 4]) = [0.0, 0.5, 0.5]\r\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDNzdZHVC1ys",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed55b8ee-9af7-4e2c-8272-6ca2ae1c6db4"
      },
      "source": [
        "from torch.nn.functional import softmax\n",
        "\n",
        "print(attn_scores)\n",
        "attn_scores_softmax = softmax(attn_scores, dim=-1)\n",
        "print(attn_scores_softmax)\n",
        "# tensor([[6.3379e-02, 4.6831e-01, 4.6831e-01],\n",
        "#         [6.0337e-06, 9.8201e-01, 1.7986e-02],\n",
        "#         [2.9539e-04, 8.8054e-01, 1.1917e-01]])\n",
        "\n",
        "# For readability, approximate the above as follows\n",
        "attn_scores_softmax = [\n",
        "  [0.0, 0.5, 0.5],\n",
        "  [0.0, 1.0, 0.0],\n",
        "  [0.0, 0.9, 0.1]\n",
        "]\n",
        "attn_scores_softmax = torch.tensor(attn_scores_softmax)\n",
        "print(attn_scores_softmax)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 2.,  4.,  4.],\n",
            "        [ 4., 16., 12.],\n",
            "        [ 4., 12., 10.]])\n",
            "tensor([[6.3379e-02, 4.6831e-01, 4.6831e-01],\n",
            "        [6.0337e-06, 9.8201e-01, 1.7986e-02],\n",
            "        [2.9539e-04, 8.8054e-01, 1.1917e-01]])\n",
            "tensor([[0.0000, 0.5000, 0.5000],\n",
            "        [0.0000, 1.0000, 0.0000],\n",
            "        [0.0000, 0.9000, 0.1000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rqn7OqzDAW4H",
        "outputId": "82e52b91-defe-44f1-cbaa-62d815acff63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "attn_scores1 = [\r\n",
        "  [ 2.,  4.,  4.],\r\n",
        "  [ 4., 16., 12.],\r\n",
        "  [ 4., 12., 10.]\r\n",
        "]\r\n",
        "\r\n",
        "attn_scores_softmax1 = np.exp(attn_scores1) / np.sum(np.exp(attn_scores1))\r\n",
        "print(attn_scores_softmax1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[8.00212069e-07 5.91281187e-06 5.91281187e-06]\n",
            " [5.91281187e-06 9.62338462e-01 1.76258438e-02]\n",
            " [5.91281187e-06 1.76258438e-02 2.38539856e-03]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBe71nseDBhb"
      },
      "source": [
        "## Step 6: Multiply scores with values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrpU6vXYBfYZ"
      },
      "source": [
        "![texto alternativo](https://miro.medium.com/max/1973/1*9cTaJGgXPbiJ4AOCc6QHyA.gif)\r\n",
        "\r\n",
        "The softmaxed attention scores for each input (blue) is multiplied with its corresponding **value** (purple). This results in 3 alignment vectors (yellow). In this tutorial, weâ€™ll refer to them as **weighted values**.\r\n",
        "```\r\n",
        "1: 0.0 * [1, 2, 3] = [0.0, 0.0, 0.0]\r\n",
        "2: 0.5 * [2, 8, 0] = [1.0, 4.0, 0.0]\r\n",
        "3: 0.5 * [2, 6, 3] = [1.0, 3.0, 1.5]\r\n",
        "\r\n",
        "(3x1).T  * (3x3)   = (3x3) \r\n",
        "\r\n",
        "transpose(softmax_attention_score) * value metrix = weighted values\r\n",
        "``` \r\n",
        "\r\n",
        "Let's calculate it all together.\r\n",
        "\r\n",
        "```\r\n",
        "[[[0.0],   * [[[1., 2., 3.]], = [0.0, 0.0, 0.0]\r\n",
        "  [0.0],   *  [[2., 8., 0.]], = [1.0, 4.0, 0.0]\r\n",
        "  [0.0]],  *  [[2., 6., 3.]]] = [1.0, 3.0, 1.5]\r\n",
        " [[0.5],\r\n",
        "  [1.0],\r\n",
        "  [0.9]],\r\n",
        " [[0.5],\r\n",
        "  [0.0],\r\n",
        "  [0.1]]\r\n",
        "]\r\n",
        "\r\n",
        "(3x3x1).T  * (3x1x3)   = (3x3) \r\n",
        "(3x3x1)    * (3x1x3)   = (3x3) \r\n",
        "\r\n",
        "transpose(softmax_attention_score) * value metrix = weighted values\r\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNnx-Fx5DFDi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be91ce45-df93-44c2-c650-7500e4832c71"
      },
      "source": [
        "weighted_values = values[:,None] * attn_scores_softmax.T[:,:,None]\n",
        "print(weighted_values)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "        [[1.0000, 4.0000, 0.0000],\n",
            "         [2.0000, 8.0000, 0.0000],\n",
            "         [1.8000, 7.2000, 0.0000]],\n",
            "\n",
            "        [[1.0000, 3.0000, 1.5000],\n",
            "         [0.0000, 0.0000, 0.0000],\n",
            "         [0.2000, 0.6000, 0.3000]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO4JDlWVQbR9",
        "outputId": "abe6e4f7-dc31-4dbe-8e3a-3c33d0b113b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "weighted_values.shape"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM9kmBUTGO4L",
        "outputId": "261891e5-d0ce-4a66-c012-79a1cb49f90f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "values[:, None]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1., 2., 3.]],\n",
              "\n",
              "        [[2., 8., 0.]],\n",
              "\n",
              "        [[2., 6., 3.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrUyvbaBGxqD",
        "outputId": "f0bf716b-214b-486b-e04f-189d481dc364",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "values[:, None].shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 1, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rClqJuBfN5in",
        "outputId": "1ddb6ac2-4545-406c-ac98-62b543725064",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "attn_scores_softmax.T[:, :, None]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.0000],\n",
              "         [0.0000],\n",
              "         [0.0000]],\n",
              "\n",
              "        [[0.5000],\n",
              "         [1.0000],\n",
              "         [0.9000]],\n",
              "\n",
              "        [[0.5000],\n",
              "         [0.0000],\n",
              "         [0.1000]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QL_Vab9mGS3R",
        "outputId": "34201883-a92e-4144-cc29-63568c63307a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "attn_scores_softmax.T[:, :, None].shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 3, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gU6w0U9ADQIc"
      },
      "source": [
        "## Step 7: Sum weighted values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsTQjDpvT510"
      },
      "source": [
        "![texto alternativo](https://miro.medium.com/max/1973/1*1je5TwhVAwwnIeDFvww3ew.gif)\r\n",
        "\r\n",
        "Take all the **weighted values** (yellow) and sum them element-wise:\r\n",
        "\r\n",
        "```\r\n",
        "  [0.0, 0.0, 0.0]\r\n",
        "+ [1.0, 4.0, 0.0]\r\n",
        "+ [1.0, 3.0, 1.5]\r\n",
        "-----------------\r\n",
        "= [2.0, 7.0, 1.5]\r\n",
        "```\r\n",
        "\r\n",
        "The resulting vector ```[2.0, 7.0, 1.5]``` (dark green) **is Output 1**, which is based on the **query representation from Input 1** interacting with all other keys, including itself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3yNYDUEgAos"
      },
      "source": [
        "## Step 8: Repeat for Input 2 & Input 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUXFczX-Vy7Z"
      },
      "source": [
        "![texto alternativo](https://miro.medium.com/max/1973/1*G8thyDVqeD8WHim_QzjvFg.gif)\r\n",
        "\r\n",
        "Note: *The dimension of **query** and **key** must always be the same because of the dot product score function. However, the dimension of **value** may be different from **query** and **key**. The resulting output will consequently follow the dimension of **value**.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6excNSUDRRj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a7f81a7-728b-4b96-9f9b-cc8d8d5c8210"
      },
      "source": [
        "outputs = weighted_values.sum(dim=0)\n",
        "print(outputs)\n",
        "\n",
        "# tensor([[2.0000, 7.0000, 1.5000],  # Output 1\n",
        "#         [2.0000, 8.0000, 0.0000],  # Output 2\n",
        "#         [2.0000, 7.8000, 0.3000]]) # Output 3"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([2.0000, 8.0000, 0.3000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oavQirdbhAK7"
      },
      "source": [
        "## Bonus: Tensorflow 2 implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "575q0u_ahP-6"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Lo1_JI9WNku"
      },
      "source": [
        "### Step 1: Prepare inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vjwwEKMhqmZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baeecd55-b727-496c-be81-018f681f2d4f"
      },
      "source": [
        "x = [\n",
        "  [1, 0, 1, 0], # Input 1\n",
        "  [0, 2, 0, 2], # Input 2\n",
        "  [1, 1, 1, 1]  # Input 3\n",
        " ]\n",
        "\n",
        "x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
        "print(x)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[1. 0. 1. 0.]\n",
            " [0. 2. 0. 2.]\n",
            " [1. 1. 1. 1.]], shape=(3, 4), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUS8Dwf5WUF7"
      },
      "source": [
        "### Step 2: Initialise weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TN-pri7rhwJ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c495967-9a5b-4b69-ae87-d517c3417f14"
      },
      "source": [
        "w_key = [\n",
        "  [0, 0, 1],\n",
        "  [1, 1, 0],\n",
        "  [0, 1, 0],\n",
        "  [1, 1, 0]\n",
        "]\n",
        "w_query = [\n",
        "  [1, 0, 1],\n",
        "  [1, 0, 0],\n",
        "  [0, 0, 1],\n",
        "  [0, 1, 1]\n",
        "]\n",
        "w_value = [\n",
        "  [0, 2, 0],\n",
        "  [0, 3, 0],\n",
        "  [1, 0, 3],\n",
        "  [1, 1, 0]\n",
        "]\n",
        "w_key = tf.convert_to_tensor(w_key, dtype=tf.float32)\n",
        "w_query = tf.convert_to_tensor(w_query, dtype=tf.float32)\n",
        "w_value = tf.convert_to_tensor(w_value, dtype=tf.float32)\n",
        "print(\"Weights for key: \\n\", w_key)\n",
        "print(\"Weights for query: \\n\", w_query)\n",
        "print(\"Weights for value: \\n\", w_value)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights for key: \n",
            " tf.Tensor(\n",
            "[[0. 0. 1.]\n",
            " [1. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 1. 0.]], shape=(4, 3), dtype=float32)\n",
            "Weights for query: \n",
            " tf.Tensor(\n",
            "[[1. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 1.]], shape=(4, 3), dtype=float32)\n",
            "Weights for value: \n",
            " tf.Tensor(\n",
            "[[0. 2. 0.]\n",
            " [0. 3. 0.]\n",
            " [1. 0. 3.]\n",
            " [1. 1. 0.]], shape=(4, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6YaUrjgWf4Q"
      },
      "source": [
        "### Step 3: Derive key, query and value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jp2DP46Sh19r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b9f99f9-45df-4a5e-af50-12dce26d326a"
      },
      "source": [
        "keys = tf.matmul(x, w_key)\n",
        "querys = tf.matmul(x, w_query)\n",
        "values = tf.matmul(x, w_value)\n",
        "print(keys)\n",
        "print(querys)\n",
        "print(values)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[0. 1. 1.]\n",
            " [4. 4. 0.]\n",
            " [2. 3. 1.]], shape=(3, 3), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[1. 0. 2.]\n",
            " [2. 2. 2.]\n",
            " [2. 1. 3.]], shape=(3, 3), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[1. 2. 3.]\n",
            " [2. 8. 0.]\n",
            " [2. 6. 3.]], shape=(3, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xKSLGcGWm11"
      },
      "source": [
        "### Step 4: Calculate attention scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLJDo_bFigkm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0765cb5-f83e-414a-e461-c3c56ac8ac3d"
      },
      "source": [
        "attn_scores = tf.matmul(querys, keys, transpose_b=True)\n",
        "print(attn_scores)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[ 2.  4.  4.]\n",
            " [ 4. 16. 12.]\n",
            " [ 4. 12. 10.]], shape=(3, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hK8ajk0jWwDF"
      },
      "source": [
        "### Step 5: Calculate softmax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QY858MEiibV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33e0f919-1dfb-434b-ae1a-42623e17bc8f"
      },
      "source": [
        "attn_scores_softmax = tf.nn.softmax(attn_scores, axis=-1)\n",
        "print(attn_scores_softmax)\n",
        "\n",
        "# For readability, approximate the above as follows\n",
        "attn_scores_softmax = [\n",
        "  [0.0, 0.5, 0.5],\n",
        "  [0.0, 1.0, 0.0],\n",
        "  [0.0, 0.9, 0.1]\n",
        "]\n",
        "attn_scores_softmax = tf.convert_to_tensor(attn_scores_softmax)\n",
        "print(attn_scores_softmax)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[6.3378938e-02 4.6831051e-01 4.6831051e-01]\n",
            " [6.0336647e-06 9.8200780e-01 1.7986100e-02]\n",
            " [2.9538720e-04 8.8053685e-01 1.1916770e-01]], shape=(3, 3), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[0.  0.5 0.5]\n",
            " [0.  1.  0. ]\n",
            " [0.  0.9 0.1]], shape=(3, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFw9oU5OW8RG"
      },
      "source": [
        "### Step 6: Multiply scores with values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOJMfkFpi0KQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b930c1b-c02c-41b1-a778-99cd764d900e"
      },
      "source": [
        "weighted_values = values[:, None] * tf.transpose(attn_scores_softmax)[:, :, None]\n",
        "print(weighted_values)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[[0.  0.  0. ]\n",
            "  [0.  0.  0. ]\n",
            "  [0.  0.  0. ]]\n",
            "\n",
            " [[1.  4.  0. ]\n",
            "  [2.  8.  0. ]\n",
            "  [1.8 7.2 0. ]]\n",
            "\n",
            " [[1.  3.  1.5]\n",
            "  [0.  0.  0. ]\n",
            "  [0.2 0.6 0.3]]], shape=(3, 3, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96AyBwVJYVmG"
      },
      "source": [
        "### Step 7: Sum weighted values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jan_cyy7i-s7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25e10cdf-ed68-4675-83b6-4398fb9fd3a6"
      },
      "source": [
        "outputs = tf.reduce_sum(weighted_values, axis=0)  # 6\n",
        "print(outputs)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[2.        7.        1.5      ]\n",
            " [2.        8.        0.       ]\n",
            " [2.        7.7999997 0.3      ]], shape=(3, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8_2vr48YtcP"
      },
      "source": [
        ">**Notes**\r\n",
        "\r\n",
        ">The dimension of query and key must always be the same because of the dot product score function. However, the dimension of value may be different from query and key. The resulting output will consequently follow the dimension of value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbjZsLf7Zpw2"
      },
      "source": [
        "##Extending to Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4pYs2D3Zq-R"
      },
      "source": [
        "So, where do we go from here? Transformers! Indeed we live in exciting times of deep learning research and high compute resources. Transformer is the incarnation from [Attention Is All You Need](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3), orginally born to perform neural machine translation. Researchers picked up from here, reassembling, cutting, adding and extending the parts, and extend its usage to more language tasks.\r\n",
        "\r\n",
        "Here I will briefly mention how we can extend self-attention to a Transformer architecture.\r\n",
        "\r\n",
        "Within the self-attention module:\r\n",
        "- Dimension\r\n",
        "- Bias\r\n",
        "\r\n",
        "Inputs to the self-attention module:\r\n",
        "- Embedding module\r\n",
        "- Positional encoding\r\n",
        "- Truncating\r\n",
        "- Masking\r\n",
        "\r\n",
        "Adding more self-attention modules:\r\n",
        "\r\n",
        "- Multihead\r\n",
        "- Layer stacking\r\n",
        "\r\n",
        "Modules between self-attention modules:\r\n",
        "\r\n",
        "- Linear transformations\r\n",
        "- LayerNorm\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIkFko9IaeVF"
      },
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJAN2Htmafnp"
      },
      "source": [
        "[Attention Is All You Need](https://arxiv.org/abs/1706.03762) (arxiv.org)\r\n",
        "\r\n",
        "[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) (jalammar.github.io)\r\n",
        "\r\n",
        "[Attn: Illustrated Attention](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3) (towardsdatascience.com)"
      ]
    }
  ]
}