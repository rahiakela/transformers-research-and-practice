{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-slice-and-dice-dataset.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOELUmWbINEuFifW9uUSmmd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/transformers-research-and-practice/blob/main/huggingface-transformers/huggingface-course/05-dataset-library/2_slice_and_dice_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LB2sNi7K5Anv"
      },
      "source": [
        "##Slice and dice the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HZjNHqB5F6Y"
      },
      "source": [
        "Most of the time, the data you work with won‚Äôt be perfectly prepared for training models. In this section we‚Äôll explore the various features that ü§ó Datasets provides to clean up your datasets.\n",
        "\n",
        "Let's install the Transformers and Datasets libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naCGRV1T5S_D"
      },
      "source": [
        "!pip -q install datasets transformers[sentencepiece]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H39XwnBa5V6K"
      },
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import Dataset\n",
        "from datasets import load_from_disk\n",
        "\n",
        "import html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igxHTIj-9JTi"
      },
      "source": [
        "##Slicing and dicing our data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VwFE9CR9KSC"
      },
      "source": [
        "Similar to Pandas, ü§ó Datasets provides several functions to manipulate the contents of Dataset and DatasetDict objects. We already encountered the Dataset.map() method in Chapter 3, and in this section we‚Äôll explore some of the other functions at our disposal.\n",
        "\n",
        "For this example we‚Äôll use the [Drug Review Dataset](https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Drugs.com%29) that‚Äôs hosted on the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php), which contains patient reviews on various drugs, along with the condition being treated and a 10-star rating of the patient‚Äôs satisfaction.\n",
        "\n",
        "First we need to download and extract the data, which can be done with the wget and unzip commands:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAsZRGnp9YkZ",
        "outputId": "97b5cfd8-bea6-4799-888e-c1c2b71c2760"
      },
      "source": [
        "%%shell\n",
        "\n",
        "wget -q \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\"\n",
        "unzip drugsCom_raw.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  drugsCom_raw.zip\n",
            "  inflating: drugsComTest_raw.tsv    \n",
            "  inflating: drugsComTrain_raw.tsv   \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgYcC6aw-JHt"
      },
      "source": [
        "Since TSV is just a variant of CSV that uses tabs instead of commas as the separator, we can load these files by using the csv loading script and specifying the delimiter argument in the load_dataset() function as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnCmL22i-Jku"
      },
      "source": [
        "data_files = {\n",
        "    \"train\": \"drugsComTrain_raw.tsv\",\n",
        "    \"test\": \"drugsComTest_raw.tsv\"\n",
        "}\n",
        "\n",
        "# \\t is the tab character in Python\n",
        "drug_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0q_hMok-ncA",
        "outputId": "5224060a-cf60-43f2-a411-9f5df419a4c1"
      },
      "source": [
        "drug_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['Unnamed: 0', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
              "        num_rows: 161297\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['Unnamed: 0', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
              "        num_rows: 53766\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZpWTWKC-xZO"
      },
      "source": [
        "A good practice when doing any sort of data analysis is to grab a small random sample to get a quick feel for the type of data you‚Äôre working with. \n",
        "\n",
        "In ü§ó Datasets, we can create a random sample by chaining the Dataset.shuffle() and Dataset.select() functions together:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xngMlJHn-ywo"
      },
      "source": [
        "drug_sample = drug_dataset[\"train\"].shuffle(seed=42).select(range(1000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIlzkQ5P_Fs4",
        "outputId": "eb2d1516-6785-49fc-ff10-afafcd25fe3a"
      },
      "source": [
        "# Peek at the first few examples\n",
        "drug_sample[:3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Unnamed: 0': [87571, 178045, 80482],\n",
              " 'condition': ['Gout, Acute', 'ibromyalgia', 'Inflammatory Conditions'],\n",
              " 'date': ['September 2, 2015', 'November 7, 2011', 'June 5, 2013'],\n",
              " 'drugName': ['Naproxen', 'Duloxetine', 'Mobic'],\n",
              " 'rating': [9.0, 3.0, 10.0],\n",
              " 'review': ['\"like the previous person mention, I&#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refills.....Aleve works!\"',\n",
              "  '\"I have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\\r\\nas a pain reducer and an anti-depressant, however, the side effects outweighed \\r\\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\\r\\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\\r\\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\\r\\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects.\"',\n",
              "  '\"I have been taking Mobic for over a year with no side effects other than an elevated blood pressure.  I had severe knee and ankle pain which completely went away after taking Mobic.  I attempted to stop the medication however pain returned after a few days.\"'],\n",
              " 'usefulCount': [36, 13, 128]}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-E7SjQt_mBd"
      },
      "source": [
        "Note that we‚Äôve fixed the seed in Dataset.shuffle() for reproducibility purposes. Dataset.select() expects an iterable of indices, so we‚Äôve passed range(1000) to grab the first 1,000 examples from the shuffled dataset. From this sample we can already see a few quirks in our dataset:\n",
        "\n",
        "- The Unnamed: 0 column looks suspiciously like an anonymized ID for each patient.\n",
        "- The condition column includes a mix of uppercase and lowercase labels.\n",
        "- The reviews are of varying length and contain a mix of Python line separators (\\r\\n) as well as HTML character codes like &\\#039;.\n",
        "\n",
        "Let‚Äôs see how we can use ü§ó Datasets to deal with each of these issues. To test the patient ID hypothesis for the Unnamed: 0 column, we can use the Dataset.unique() function to verify that the number of IDs matches the number of rows in each split:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZ_V70m4_yeU"
      },
      "source": [
        "for split in drug_dataset.keys():\n",
        "  assert len(drug_dataset[split]) == len(drug_dataset[split].unique(\"Unnamed: 0\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2YnOCqjBeuW"
      },
      "source": [
        "This seems to confirm our hypothesis, so let‚Äôs clean up the dataset a bit by renaming the Unnamed: 0 column to something a bit more interpretable. \n",
        "\n",
        "We can use the DatasetDict.rename_column() function to rename the column across both splits in one go:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsPFts8pBgEH",
        "outputId": "84e2958c-772c-48e8-83e2-6dc221749d2e"
      },
      "source": [
        "drug_dataset = drug_dataset.rename_column(original_column_name=\"Unnamed: 0\", new_column_name=\"patient_id\")\n",
        "drug_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
              "        num_rows: 161297\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
              "        num_rows: 53766\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8VwTOKKCJNc"
      },
      "source": [
        ">‚úèÔ∏è Try it out! Use the Dataset.unique() function to find the number of unique drugs and conditions in the training and test sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q387xjmlCKrU",
        "outputId": "b0c508c7-20ec-4cfb-9ec9-fded1759ea9a"
      },
      "source": [
        "len(drug_dataset[\"train\"].unique(\"condition\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "885"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSX7iY3GDM2w",
        "outputId": "e65c4384-f46a-4574-ccb9-abbcb1fd90e2"
      },
      "source": [
        "len(drug_dataset[\"test\"].unique(\"condition\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "709"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIlwT0S2DDWM",
        "outputId": "8d69d130-042a-4a95-b1dd-98eed3983c66"
      },
      "source": [
        "len(drug_dataset[\"train\"].unique(\"drugName\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3436"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rqXQjnZDQna",
        "outputId": "3d7db937-60fe-4d09-c024-99dc8aee089e"
      },
      "source": [
        "len(drug_dataset[\"test\"].unique(\"drugName\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2637"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAAlxaB5DaX3",
        "outputId": "487f5a83-f364-4d98-9181-a23e1ea2efd6"
      },
      "source": [
        "drug_sample[:3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Unnamed: 0': [87571, 178045, 80482],\n",
              " 'condition': ['Gout, Acute', 'ibromyalgia', 'Inflammatory Conditions'],\n",
              " 'date': ['September 2, 2015', 'November 7, 2011', 'June 5, 2013'],\n",
              " 'drugName': ['Naproxen', 'Duloxetine', 'Mobic'],\n",
              " 'rating': [9.0, 3.0, 10.0],\n",
              " 'review': ['\"like the previous person mention, I&#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refills.....Aleve works!\"',\n",
              "  '\"I have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\\r\\nas a pain reducer and an anti-depressant, however, the side effects outweighed \\r\\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\\r\\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\\r\\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\\r\\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects.\"',\n",
              "  '\"I have been taking Mobic for over a year with no side effects other than an elevated blood pressure.  I had severe knee and ankle pain which completely went away after taking Mobic.  I attempted to stop the medication however pain returned after a few days.\"'],\n",
              " 'usefulCount': [36, 13, 128]}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq-bj1YZDc53"
      },
      "source": [
        "Next, let‚Äôs normalize all the condition labels using Dataset.map().\n",
        "\n",
        "As we did with tokenization in Chapter 3, we can define a simple function that can be applied across all the rows of each split in drug_dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6GSkUS-DeGT"
      },
      "source": [
        "def lowercase_condition(example):\n",
        "  return {\"condition\": example[\"condition\"].lower()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFxfwC2dDrK5"
      },
      "source": [
        "#drug_dataset.map(lowercase_condition)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3C_IvB_MN89"
      },
      "source": [
        "```log\n",
        "---------------------------------------------------------------------------\n",
        "AttributeError                            Traceback (most recent call last)\n",
        "<ipython-input-16-0678ead08fa2> in <module>()\n",
        "----> 1 drug_dataset.map(lowercase_condition)\n",
        "\n",
        "9 frames\n",
        "<ipython-input-15-a3533e5f001f> in lowercase_condition(example)\n",
        "      1 def lowercase_condition(example):\n",
        "----> 2   return {\"condition\": example[\"condition\"].lower()}\n",
        "\n",
        "AttributeError: 'NoneType' object has no attribute 'lower'\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssx9So4PD8e5"
      },
      "source": [
        "Oh no, we‚Äôve run into a problem with our map function! From the error we can infer that some of the entries in the condition column are None, which cannot be lowercased as they‚Äôre not strings. \n",
        "\n",
        "Let‚Äôs drop these rows using Dataset.filter(), which works in a similar way to Dataset.map() and expects a function that receives a single example of the dataset. Instead of writing an explicit function like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjs6pVuRD9yb"
      },
      "source": [
        "def filter_nones(x):\n",
        "  return x[\"condition\"] is not None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-mTzAl-EKsP"
      },
      "source": [
        "and then running drug_dataset.filter(filter_nones), we can do this in one line using a lambda function. In Python, lambda functions are small functions that you can define without explicitly naming them. They take the general form:\n",
        "\n",
        "```python\n",
        "lambda <arguments> : <expression>\n",
        "```\n",
        "\n",
        "where lambda is one of Python‚Äôs special keywords, <arguments> is a list/set of comma-separated values that define the inputs to the function, and <expression> represents the operations you wish to execute. \n",
        "\n",
        "For example, we can define a simple lambda function that squares a number as follows:\n",
        "\n",
        "```python\n",
        "lambda x : x * x\n",
        "```\n",
        "\n",
        "To apply this function to an input, we need to wrap it and the input in parentheses:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lq6FAz9yEgpd",
        "outputId": "b36d803b-28bb-4f54-b89a-018087abc9ae"
      },
      "source": [
        "(lambda x: x * x)(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTMOgFS5Ep3M"
      },
      "source": [
        "Similarly, we can define lambda functions with multiple arguments by separating them with commas. \n",
        "\n",
        "For example, we can compute the area of a triangle as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCDVL-AdEWG3",
        "outputId": "6b211578-9a64-48ca-c92f-188ee9734c11"
      },
      "source": [
        "(lambda base, height: 0.5 * base * height)(4, 8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16.0"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6mm8u_wE-qy"
      },
      "source": [
        "Lambda functions are handy when you want to define small, single-use functions (for more information about them, we recommend reading the excellent [Real Python tutorial by Andre Burgaud](https://realpython.com/python-lambda/)). \n",
        "\n",
        "In the ü§ó Datasets context, we can use lambda functions to define simple map and filter operations, so let‚Äôs use this trick to eliminate the None entries in our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOvx3iEIFF_F"
      },
      "source": [
        "drug_dataset = drug_dataset.filter(lambda x: x[\"condition\"] is not None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmlZDrNuFWJI"
      },
      "source": [
        "With the None entries removed, we can normalize our condition column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRt0P5UfFWkO"
      },
      "source": [
        "drug_dataset = drug_dataset.map(lowercase_condition)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8UJM7UeFeR9",
        "outputId": "8e415ba5-831b-438f-d548-901fe7061544"
      },
      "source": [
        "# Check that lowercasing worked\n",
        "drug_dataset[\"train\"][\"condition\"][:3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['left ventricular dysfunction', 'adhd', 'birth control']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFzVXmEoFoRH"
      },
      "source": [
        "It works! Now that we‚Äôve cleaned up the labels, let‚Äôs take a look at cleaning up the reviews themselves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VwRgS1nFo6i"
      },
      "source": [
        "##Creating new columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOJo2_SJFtdr"
      },
      "source": [
        "Whenever you‚Äôre dealing with customer reviews, a good practice is to check the number of words in each review. A review might be just a single word like ‚ÄúGreat!‚Äù or a full-blown essay with thousands of words, and depending on the use case you‚Äôll need to handle these extremes differently. To compute the number of words in each review, we‚Äôll use a rough heuristic based on splitting each text by whitespace.\n",
        "\n",
        "Let‚Äôs define a simple function that counts the number of words in each review:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7MMSA3-FnaY"
      },
      "source": [
        "def compute_review_length(example):\n",
        "  return {\"review_length\": len(example[\"review\"].split())}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Twt88efWBZtB"
      },
      "source": [
        "Unlike our lowercase_condition() function, compute_review_length() returns a dictionary whose key does not correspond to one of the column names in the dataset. In this case, when compute_review_length() is passed to Dataset.map(), it will be applied to all the rows in the dataset to create a new review_length column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFDTORNMBaHB"
      },
      "source": [
        "drug_dataset = drug_dataset.map(compute_review_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQrIdQxUBkzH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2468377b-881a-443d-8a31-66269b78432b"
      },
      "source": [
        "# Inspect the first training example\n",
        "drug_dataset[\"train\"][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'condition': 'left ventricular dysfunction',\n",
              " 'date': 'May 20, 2012',\n",
              " 'drugName': 'Valsartan',\n",
              " 'patient_id': 206461,\n",
              " 'rating': 9.0,\n",
              " 'review': '\"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil\"',\n",
              " 'review_length': 17,\n",
              " 'usefulCount': 27}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZeR9G-8BzFH"
      },
      "source": [
        "As expected, we can see a review_length column has been added to our training set. We can sort this new column with Dataset.sort() to see what the extreme values look like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNeAwexmBzei",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "069207ae-8b88-4d5c-9b21-e3eda21f867a"
      },
      "source": [
        "drug_dataset[\"train\"].sort(\"review_length\")[:3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'condition': ['birth control', 'muscle spasm', 'pain'],\n",
              " 'date': ['November 4, 2008', 'March 24, 2017', 'August 20, 2016'],\n",
              " 'drugName': ['Loestrin 21 1 / 20', 'Chlorzoxazone', 'Nucynta'],\n",
              " 'patient_id': [103488, 23627, 20558],\n",
              " 'rating': [10.0, 1.0, 6.0],\n",
              " 'review': ['\"Excellent.\"', '\"useless\"', '\"ok\"'],\n",
              " 'review_length': [1, 1, 1],\n",
              " 'usefulCount': [5, 2, 10]}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjgcXCzLCEs1"
      },
      "source": [
        "As we suspected, some reviews contain just a single word, which, although it may be okay for sentiment analysis, would not be informative if we want to predict the condition.\n",
        "\n",
        ">üôã An alternative way to add new columns to a dataset is with the Dataset.add_column() function. This allows you to provide the column as a Python list or NumPy array and can be handy in situations where Dataset.map() is not well suited for your analysis.\n",
        "\n",
        "Let‚Äôs use the Dataset.filter() function to remove reviews that contain fewer than 30 words. Similarly to what we did with the condition column, we can filter out the very short reviews by requiring that the reviews have a length above this threshold:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6bLE8hMCQ78"
      },
      "source": [
        "drug_dataset = drug_dataset.filter(lambda x: x[\"review_length\"] > 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4gabrFSCaPo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "215bc0bd-e7aa-4dfe-c428-58e6408c4cde"
      },
      "source": [
        "print(drug_dataset.num_rows)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': 138514, 'test': 46108}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYHoIKjPClPh"
      },
      "source": [
        "As you can see, this has removed around 15% of the reviews from our original training and test sets.\n",
        "\n",
        ">‚úèÔ∏è Try it out! Use the Dataset.sort() function to inspect the reviews with the largest numbers of words. [See the documentation](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.sort) to see which argument you need to use sort the reviews by length in descending order.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJ2ruObqDEtx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c19a54ff-a04e-44cb-96bc-972b037cbc8d"
      },
      "source": [
        "drug_dataset[\"train\"].sort(\"review_length\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
              "    num_rows: 138514\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISazGs36Elfj"
      },
      "source": [
        "The last thing we need to deal with is the presence of HTML character codes in our reviews. We can use Python‚Äôs html module to unescape these characters, like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqgyNy4sEmC_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6747b23-d3e8-4a74-a963-ff4ffbb4f668"
      },
      "source": [
        "text = \"I&#039;m a transformer called BERT\"\n",
        "html.unescape(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I'm a transformer called BERT\""
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E8mSS9rLf44"
      },
      "source": [
        "We‚Äôll use Dataset.map() to unescape all the HTML characters in our corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjlgB_mmLgSm"
      },
      "source": [
        "drug_dataset = drug_dataset.map(lambda x: {\"review\": html.unescape(x[\"review\"])})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J--gMWTXL2Q-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78253a59-69f1-4351-e6e8-1bcc27a921ff"
      },
      "source": [
        "drug_dataset[\"train\"][\"review\"][:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\"My son is halfway through his fourth week of Intuniv. We became concerned when he began this last week, when he started taking the highest dose he will be on. For two days, he could hardly get out of bed, was very cranky, and slept for nearly 8 hours on a drive home from school vacation (very unusual for him.) I called his doctor on Monday morning and she said to stick it out a few days. See how he did at school, and with getting up in the morning. The last two days have been problem free. He is MUCH more agreeable than ever. He is less emotional (a good thing), less cranky. He is remembering all the things he should. Overall his behavior is better. \\r\\nWe have tried many different medications and so far this is the most effective.\"',\n",
              " '\"I used to take another oral contraceptive, which had 21 pill cycle, and was very happy- very light periods, max 5 days, no other side effects. But it contained hormone gestodene, which is not available in US, so I switched to Lybrel, because the ingredients are similar. When my other pills ended, I started Lybrel immediately, on my first day of period, as the instructions said. And the period lasted for two weeks. When taking the second pack- same two weeks. And now, with third pack things got even worse- my third period lasted for two weeks and now it\\'s the end of the third week- I still have daily brown discharge.\\r\\nThe positive side is that I didn\\'t have any other side effects. The idea of being period free was so tempting... Alas.\"',\n",
              " '\"This is my first time using any form of birth control. I\\'m glad I went with the patch, I have been on it for 8 months. At first It decreased my libido but that subsided. The only downside is that it made my periods longer (5-6 days to be exact) I used to only have periods for 3-4 days max also made my cramps intense for the first two days of my period, I never had cramps before using birth control. Other than that in happy with the patch\"',\n",
              " '\"Suboxone has completely turned my life around.  I feel healthier, I\\'m excelling at my job and I always have money in my pocket and my savings account.  I had none of those before Suboxone and spent years abusing oxycontin.  My paycheck was already spent by the time I got it and I started resorting to scheming and stealing to fund my addiction.  All that is history.  If you\\'re ready to stop, there\\'s a good chance that suboxone will put you on the path of great life again.  I have found the side-effects to be minimal compared to oxycontin.  I\\'m actually sleeping better.   Slight constipation is about it for me.  It truly is amazing. The cost pales in comparison to what I spent on oxycontin.\"',\n",
              " '\"2nd day on 5mg started to work with rock hard erections however experianced headache, lower bowel preassure. 3rd day erections would wake me up & hurt! Leg/ankles aches   severe lower bowel preassure like you need to go #2 but can\\'t! Enjoyed the initial rockhard erections but not at these side effects or $230 for months supply! I\\'m 50 & work out 3Xs a week. Not worth side effects!\"']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXp77R-LLxSx"
      },
      "source": [
        "As you can see, the Dataset.map() method is quite useful for processing data ‚Äî and we haven‚Äôt even scratched the surface of everything it can do!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thZXnjl5LyEr"
      },
      "source": [
        "##The map() method‚Äôs superpowers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUd61Q1dL0aM"
      },
      "source": [
        "The Dataset.map() method takes a batched argument that, if set to True, causes it to send a batch of examples to the map function at once (the batch size is configurable but defaults to 1,000). For instance, the previous map function that unescaped all the HTML took a bit of time to run (you can read the time taken from the progress bars). We can speed this up by processing several elements at the same time using a list comprehension.\n",
        "\n",
        "When you specify batched=True the function receives a dictionary with the fields of the dataset, but each value is now a list of values, and not just a single value. The return value of Dataset.map() should be the same: a dictionary with the fields we want to update or add to our dataset, and a list of values. \n",
        "\n",
        "For example, here is another way to unescape all HTML characters, but using batched=True:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUoriLvu2alb"
      },
      "source": [
        "new_drug_dataset = drug_dataset.map(lambda x: {\"review\": [html.unescape(o) for o in x[\"review\"]]}, batched=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2ml4ARp28k6"
      },
      "source": [
        "If you‚Äôre running this code in a notebook, you‚Äôll see that this command executes way faster than the previous one. And it‚Äôs not because our reviews have already been HTML-unescaped ‚Äî if you re-execute the instruction from the previous section (without batched=True), it will take the same amount of time as before. This is because list comprehensions are usually faster than executing the same code in a for loop, and we also gain some performance by accessing lots of elements at the same time instead of one by one.\n",
        "\n",
        "Using `Dataset.map()` with `batched=True` will be essential to unlock the speed of the ‚Äúfast‚Äù tokenizers that we‚Äôll encounter in [Chapter 6](https://huggingface.co/course/chapter6/1), which can quickly tokenize big lists of texts. For instance, to tokenize all the drug reviews with a fast tokenizer, we could use a function like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6gUeIua3G0T"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "\n",
        "def tokenize(examples):\n",
        "  return tokenizer(examples[\"review\"], truncation=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF_5T7FY3kii"
      },
      "source": [
        "As you saw in [Chapter 3](https://huggingface.co/course/chapter3/1), we can pass one or several examples to the tokenizer, so we can use this function with or without `batched=True`. \n",
        "\n",
        "Let‚Äôs take this opportunity to compare the performance of the different options. In a notebook, you can time a one-line instruction by adding %time before the line of code you wish to measure:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DypsB7sb3uJP"
      },
      "source": [
        "%time tokenized_dataset = drug_dataset.map(tokenize, batched=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y1Gx3S24MYI"
      },
      "source": [
        "You can also time a whole cell by putting %%time at the beginning of the cell. On the hardware we executed this on, it showed 10.8s for this instruction (it‚Äôs the number written after ‚ÄúWall time‚Äù)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBZDNfQt4M9s"
      },
      "source": [
        "%%time \n",
        "tokenized_dataset = drug_dataset.map(tokenize, batched=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-DZHXGH4dpT"
      },
      "source": [
        ">‚úèÔ∏è Try it out! Execute the same instruction with and without batched=True, then try it with a slow tokenizer (add use_fast=False in the `AutoTokenizer.from_pretrained()` method) so you can see what numbers you get on your hardware."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odm1ijXG4e9J"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", use_fast=False)\n",
        "\n",
        "\n",
        "def tokenize(examples):\n",
        "  return tokenizer(examples[\"review\"], truncation=True)\n",
        "\n",
        "%time tokenized_dataset = drug_dataset.map(tokenize, batched=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxO6WsiM4yXP",
        "outputId": "34b04a6f-d4fa-4024-e73d-c5d4775894b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time \n",
        "tokenized_dataset = drug_dataset.map(tokenize, batched=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-3761173c276c0a9a/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-c15ff0d1712508c0.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-3761173c276c0a9a/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-4d097ba4a4b47ad2.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 603 ms, sys: 6.96 ms, total: 610 ms\n",
            "Wall time: 616 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6nwGCvP5Ipy"
      },
      "source": [
        "ere are the results we obtained with and without batching, with a fast and a slow tokenizer:\n",
        "\n",
        "|Options|Fast tokenizer|Slow tokenizer|\n",
        "|--|--|--|\n",
        "|batched=True|10.8s|4min41s|\n",
        "|batched=False|59.2s|5min3s|\n",
        "\n",
        "This means that using a fast tokenizer with the batched=True option is 30 times faster than its slow counterpart with no batching ‚Äî this is truly amazing! That‚Äôs the main reason why fast tokenizers are the default when using AutoTokenizer (and why they are called ‚Äúfast‚Äù). They‚Äôre able to achieve such a speedup because behind the scenes the tokenization code is executed in Rust, which is a language that makes it easy to parallelize code execution.\n",
        "\n",
        "Parallelization is also the reason for the nearly 6x speedup the fast tokenizer achieves with batching: you can‚Äôt parallelize a single tokenization operation, but when you want to tokenize lots of texts at the same time you can just split the execution across several processes, each responsible for its own texts.\n",
        "\n",
        "`Dataset.map()` also has some parallelization capabilities of its own. Since they are not backed by Rust, they won‚Äôt let a slow tokenizer catch up with a fast one, but they can still be helpful (especially if you‚Äôre using a tokenizer that doesn‚Äôt have a fast version). \n",
        "\n",
        "To enable multiprocessing, use the `num_proc` argument and specify the number of processes to use in your call to `Dataset.map()`:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kiav2S77T2q"
      },
      "source": [
        "slow_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", use_fast=False)\n",
        "\n",
        "def slow_tokenize(examples):\n",
        "  return slow_tokenizer(examples[\"review\"], truncation=True)\n",
        "\n",
        "tokenized_dataset = drug_dataset.map(slow_tokenize, batched=True, num_proc=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s21IstrB8Yho"
      },
      "source": [
        "All of this functionality condensed into a single method is already pretty amazing, but there‚Äôs more! With Dataset.map() and batched=True you can change the number of elements in your dataset. This is super useful in many situations where you want to create several training features from one example, and we will need to do this as part of the preprocessing for several of the NLP tasks we‚Äôll undertake in [Chapter 7](https://huggingface.co/course/chapter7/1).\n",
        "\n",
        "\n",
        "Let‚Äôs have a look at how it works! Here we will tokenize our examples and truncate them to a maximum length of 128, but we will ask the tokenizer to return all the chunks of the texts instead of just the first one. This can be done with `return_overflowing_tokens=True`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5D3aSce8dSa"
      },
      "source": [
        "def tokenize_and_split(examples):\n",
        "  return tokenizer(examples[\"review\"], truncation=True, max_length=128, return_overflowing_tokens=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let‚Äôs test this on one example before using Dataset.map() on the whole dataset:"
      ],
      "metadata": {
        "id": "nEfiVmt6qTbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = tokenize_and_split(drug_dataset[\"train\"][0])"
      ],
      "metadata": {
        "id": "00llxkgdqUEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[inps for inps in result[\"input_ids\"]]"
      ],
      "metadata": {
        "id": "abkhtS1iraIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Now let‚Äôs do this for all elements of the dataset!"
      ],
      "metadata": {
        "id": "3LpITW3VsFZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)"
      ],
      "metadata": {
        "id": "Zyh38a11sF1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The problem is that we‚Äôre trying to mix two different datasets of different sizes.\n",
        "\n",
        " We can do with the remove_columns argument:"
      ],
      "metadata": {
        "id": "tYLrHahismuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True, remove_columns=drug_dataset[\"train\"].column_names)"
      ],
      "metadata": {
        "id": "OOWvxvJdsrIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenized_dataset[\"train\"]), len(drug_dataset[\"train\"])"
      ],
      "metadata": {
        "id": "4f5QWl3HuhA7",
        "outputId": "ac3c103c-c97d-4910-a5cc-1d7f70983e58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(138514, 138514)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_split(examples):\n",
        "  result = tokenizer(examples[\"review\"], truncation=True, max_length=128, return_overflowing_tokens=True)\n",
        "\n",
        "  # Extract mapping between new and old indices\n",
        "  sample_map = result.pop(\"overflow_to_sample_mapping\")\n",
        "  for key, value in examples.items():\n",
        "    result[key] = [value[i] for i in sample_map]\n",
        "  return result"
      ],
      "metadata": {
        "id": "-4R_iDOBvBa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can see it works with Dataset.map() without us needing to remove the old columns:\n",
        "tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)\n",
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "mLgwI7qTwvFP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}